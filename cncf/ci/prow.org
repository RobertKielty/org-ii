#+TITLE: #Prow for cncf-ci
#+AUTHOR: Hippie Hacker
#+EMAIL: hh@ii.coop
#+CREATOR: ii.coop
#+DATE: 8th of March, 2019

* Deploying Prow

Following [[https://github.com/kubernetes/test-infra/blob/master/prow/getting_started_deploy.md][prow/getting_started#Deploying Prow]]:

#+NAME: go get tackle
#+BEGIN_SRC shell :noweb yes :var tmpdir=(symbol-value 'tmpdir)
go get -u k8s.io/test-infra/prow/cmd/tackle
#+END_SRC

** cluster
:PROPERTIES:
:noheader-args:tmate: :socket "/tmp/hippie.packet-setup.iisocket"
:noheader-args:tmate: :session main:prow
:noheader-args:shell+: :dir "/ssh:root@139.178.88.146:"
:END:
[[file:~/ii/org/k8s.io/kubernetes/packet-setup.org::*TLDR][After following the TLDR to setup k8s on Packet]]:

*** ensure GCP user has cluster-admin-binding
#+NAME: giving gcloud account cluster-admin
#+BEGIN_SRC shell
kubectl create clusterrolebinding cluster-admin-binding \
  --clusterrole cluster-admin --user $(gcloud config get-value account)
#+END_SRC

#+RESULTS: giving gcloud account cluster-admin
#+BEGIN_EXAMPLE :noeval t
clusterrolebinding.rbac.authorization.k8s.io/cluster-admin-binding created
#+END_EXAMPLE

*** service account
#+NAME: Setup a Kubernetes Service Account
#+BEGIN_SRC shell
  kubectl --namespace kube-system create serviceaccount cncf-ci
  kubectl create clusterrolebinding cncf-ci \
    --clusterrole cluster-admin \
    --serviceaccount=kube-system:cncf-ci
#+END_SRC

#+RESULTS: Setup a Kubernetes Service Account
#+BEGIN_EXAMPLE :noeval t
serviceaccount/cncf-ci created
clusterrolebinding.rbac.authorization.k8s.io/cncf-ci created
#+END_EXAMPLE

#+NAME: GCLOUD_SERVICE_ACCOUNT
#+BEGIN_SRC shell
  gcloud iam service-accounts create cncf-ci || true
  gcloud iam service-accounts describe cncf-ci@apisnoop.iam.gserviceaccount.com
#+END_SRC

#+RESULTS: GCLOUD_SERVICE_ACCOUNT
#+BEGIN_EXAMPLE :noeval t
email: cncf-ci@apisnoop.iam.gserviceaccount.com
etag: MDEwMjE5MjA=
name: projects/apisnoop/serviceAccounts/cncf-ci@apisnoop.iam.gserviceaccount.com
oauth2ClientId: '117158085094240977692'
projectId: apisnoop
uniqueId: '117158085094240977692'
#+END_EXAMPLE

#+RESULTS: Setup a GCloud Service Account Secret Key
#+BEGIN_SRC shell :results silent
  SERVICE_ACCOUNT=cncf-ci@apisnoop.iam.gserviceaccount.com
  gcloud projects add-iam-policy-binding apisnoop \
           --member serviceAccount:${SERVICE_ACCOUNT} \
           --role=roles/storage.admin
#+END_SRC

#+RESULTS: Export service account key into file
#+BEGIN_SRC shell :results silent
  SERVICE_ACCOUNT=cncf-ci@apisnoop.iam.gserviceaccount.com
  gcloud iam service-accounts keys \
         create ./service-account.json \
         --iam-account=$SERVICE_ACCOUNT \
         --key-file-type=json
#+END_SRC

#+NAME: create gcloud service account k8s secret
#+BEGIN_SRC shell
  SERVICE_ACCOUNT=cncf-ci@apisnoop.iam.gserviceaccount.com
  kubectl create secret generic service-account \
    --from-file=service-account.json
#+END_SRC

#+RESULTS: create gcloud service account k8s secret
#+BEGIN_EXAMPLE :noeval t
secret/service-account created
#+END_EXAMPLE

*** github secrets
**** hmac-token
#+NAME: create github hmac-token
#+BEGIN_SRC shell
  kubectl create secret generic hmac-token \
    --from-literal=hmac=$(openssl rand -hex 20)
#+END_SRC

#+RESULTS: create github hmac-token
#+BEGIN_EXAMPLE :noeval t
secret/hmac-token created
#+END_EXAMPLE

#+NAME: github hmac-token
#+BEGIN_SRC shell :results silent
  kubectl get secret hmac-token  -o json | jq -r .data.hmac | base64 -d
#+END_SRC

**** github oauth-token to cluster

Set GITHUB_OATH_TOKEN with a token created at https://github.com/settings/tokens

#+NAME: save github oauth-token to cluster
#+BEGIN_SRC shell
  GITHUB_OAUTH_TOKEN=SET_ME
  kubectl create secret generic oauth-token \
    --from-literal=oauth=$GITHUB_OAUTH_TOKEN
#+END_SRC

#+RESULTS: save github oauth-token to cluster
#+BEGIN_EXAMPLE :noeval t
secret/oauth-token created
#+END_EXAMPLE

#+NAME: gihub oauth-token
#+BEGIN_SRC shell :results silent
kubectl get secret oauth-token  -o json | jq -r .data.oauth | base64 -d
#+END_SRC
*** deploy basic prow components
**** starter.yaml
[[https://github.com/kubernetes/test-infra/blob/master/prow/cluster/starter.yaml][prow/cluster/starter.yaml]]

#+NAME: basic prow components
#+BEGIN_SRC shell
  kubectl apply -f \
    https://raw.githubusercontent.com/kubernetes/test-infra/master/prow/cluster/starter.yaml 2>&1
  echo $?
#+END_SRC

#+RESULTS: basic prow components
#+BEGIN_EXAMPLE :noeval t
configmap/plugins created
configmap/config created
customresourcedefinition.apiextensions.k8s.io/prowjobs.prow.k8s.io created
deployment.extensions/hook created
service/hook created
deployment.extensions/plank created
deployment.extensions/sinker created
deployment.extensions/deck created
service/deck created
deployment.extensions/horologium created
deployment.extensions/tide created
service/tide created
ingress.extensions/ing created
serviceaccount/deck created
rolebinding.rbac.authorization.k8s.io/deck created
role.rbac.authorization.k8s.io/deck created
serviceaccount/horologium created
role.rbac.authorization.k8s.io/horologium created
rolebinding.rbac.authorization.k8s.io/horologium created
serviceaccount/plank created
role.rbac.authorization.k8s.io/plank created
rolebinding.rbac.authorization.k8s.io/plank created
serviceaccount/sinker created
role.rbac.authorization.k8s.io/sinker created
rolebinding.rbac.authorization.k8s.io/sinker created
serviceaccount/hook created
role.rbac.authorization.k8s.io/hook created
rolebinding.rbac.authorization.k8s.io/hook created
serviceaccount/tide created
role.rbac.authorization.k8s.io/tide created
rolebinding.rbac.authorization.k8s.io/tide created
0
#+END_EXAMPLE

#+NAME: our customized prow deployment
#+BEGIN_SRC shell
  kubectl apply -f prow.yaml | grep -v unchanged 2>&1
  echo $?
#+END_SRC

#+RESULTS: our customized prow deployment
#+BEGIN_EXAMPLE :noeval t
deployment.extensions/hook configured
deployment.extensions/plank configured
deployment.extensions/sinker configured
deployment.extensions/deck configured
deployment.extensions/horologium configured
deployment.extensions/tide configured
0
#+END_EXAMPLE


**** get deployments
#+NAME: prow components
#+BEGIN_SRC shell
  kubectl get deployments
#+END_SRC

#+RESULTS: prow components
#+BEGIN_EXAMPLE :noeval t
NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deck         2         2         2            2           21d
hook         2         2         2            2           21d
horologium   1         1         1            1           21d
plank        1         1         1            1           21d
sinker       1         1         1            1           21d
tide         1         1         1            1           21d
#+END_EXAMPLE

**** ingress ip
#+NAME: ingress ip
#+BEGIN_SRC shell
kubectl get ingress ing
#+END_SRC

#+RESULTS: ingress ip
#+BEGIN_EXAMPLE :noeval t
NAME   HOSTS   ADDRESS        PORTS   AGE
ing    *       35.241.26.71   80      2m
#+END_EXAMPLE

#+NAME: ingress ip oneliner
#+BEGIN_SRC shell
  kubectl get ingress ing -o=jsonpath='{.status.loadBalancer.ingress[0].ip}'
#+END_SRC

#+RESULTS: ingress ip oneliner
#+BEGIN_EXAMPLE :noeval t
35.241.26.71
#+END_EXAMPLE

**** ingress status
Note the status: loadbalancer:
#+NAME: ingress ing
#+BEGIN_SRC shell :results output verbatim code :wrap "SRC yaml"
kubectl get ingress ing -o yaml
#+END_SRC

#+RESULTS: ingress ing
#+BEGIN_SRC yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{},"name":"ing","namespace":"default"},"spec":{"rules":[{"http":{"paths":[{"backend":{"serviceName":"deck","servicePort":80},"path":"/*"},{"backend":{"serviceName":"hook","servicePort":8888},"path":"/hook"}]}}]}}
  creationTimestamp: "2019-03-17T17:19:31Z"
  generation: 1
  name: ing
  namespace: default
  resourceVersion: "407666"
  selfLink: /apis/extensions/v1beta1/namespaces/default/ingresses/ing
  uid: d3c8960c-48d8-11e9-87ef-42010a98000e
spec:
  rules:
  - http:
      paths:
      - backend:
          serviceName: deck
          servicePort: 80
        path: /*
      - backend:
          serviceName: hook
          servicePort: 8888
        path: /hook
status:
  loadBalancer: {}
#+END_SRC

**** get ingress yaml

I suspect we are not getting ingress IP's because the Ingress is looking for a loadBalancer.

https://github.com/kubernetes/test-infra/blob/master/prow/cluster/starter.yaml#L400

#+NAME: ing yaml
#+BEGIN_SRC yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  namespace: default
  name: ing
spec:
  rules:
  - http:
      paths:
      - path: /* # Correct for GKE, need / on many other distros
        backend:
          serviceName: deck
          servicePort: 80
      - path: /hook
        backend:
          serviceName: hook
          servicePort: 8888
#+END_SRC

*** add the webhook to github

#+NAME: install add-hook
#+BEGIN_SRC shell
  go get -u k8s.io/test-infra/experiment/add-hook
#+END_SRC

#+NAME: add-hook
#+BEGIN_SRC shell
  kubectl get secret oauth-token  -o json | jq -r .data.oauth | base64 -d > oauth-token
  kubectl get secret hmac-token  -o json | jq -r .data.hmac | base64 -d > hmac-token
  # HOOK_URL=http://$(kubectl get ingress ing -o=jsonpath='{.status.loadBalancer.ingress[0].ip}')/hook
  HOOK_URL=http://prow.cncf.ci/hook
  add-hook \
    --hmac-path=hmac-token \
    --github-token-path=oauth-token\
    --hook-url=$HOOK_URL \
    --repo ii/RapuTure \
    --confirm=true #confirm=false to dry run
#+END_SRC

#+RESULTS: add-hook
#+BEGIN_EXAMPLE :noeval t
#+END_EXAMPLE

** config.yaml
:PROPERTIES:
:header-args:yaml+: :tangle config.yaml
:END:

Initially config is empty and plugins only contains size:
*** deck
#+BEGIN_SRC yaml
  # https://github.com/kubernetes/test-infra/issues/11729
  #time="2019-03-18T07:06:59+13:00" level=fatal msg="Error loading Prow config."
  # component=checkconfig error="no default decoration config provided for plank"
  deck:
    spyglass:
      size_limit: 500e+6 # 500MB
      viewers:
        "started.json|finished.json": ["metadata"]
        "build-log.txt": ["buildlog"]
        "artifacts/junit.*\\.xml": ["junit"]
        # Remember to escape your '\' in yaml strings!
#+END_SRC
*** plank
#+BEGIN_SRC yaml
  plank:
    job_url_template: 'https://job_url_template/'
  #  job_url_template: '{{if .Spec.Refs}}{{if eq .Spec.Refs.Org "kubernetes-security"}}https://console.cloud.google.com/storage/browser/kubernetes-security-prow/{{else}}https://prow.k8s.io/view/gcs/kubernetes-jenkins/{{end}}{{else}}https://prow.k8s.io/view/gcs/kubernetes-jenkins/{{end}}{{if eq .Spec.Type "presubmit"}}pr-logs/pull{{else if eq .Spec.Type "batch"}}pr-logs/pull{{else}}logs{{end}}{{if .Spec.Refs}}{{if ne .Spec.Refs.Org ""}}{{if ne .Spec.Refs.Org "kubernetes"}}/{{if and (eq .Spec.Refs.Org "kubernetes-sigs") (ne .Spec.Refs.Repo "poseidon")}}sigs.k8s.io{{else}}{{.Spec.Refs.Org}}{{end}}_{{.Spec.Refs.Repo}}{{else if ne .Spec.Refs.Repo "kubernetes"}}/{{.Spec.Refs.Repo}}{{end}}{{end}}{{end}}{{if eq .Spec.Type "presubmit"}}/{{with index .Spec.Refs.Pulls 0}}{{.Number}}{{end}}{{else if eq .Spec.Type "batch"}}/batch{{end}}/{{.Spec.Job}}/{{.Status.BuildID}}/'
    report_template: '[Full PR test history](http://prow.cncf.ci/pr-history?org={{.Spec.Refs.Org}}&repo={{.Spec.Refs.Repo}}&pr={{with index .Spec.Refs.Pulls 0}}{{.Number}}{{end}}). [Your PR dashboard](https://gubernator.cncf.ci/pr/{{with index .Spec.Refs.Pulls 0}}{{.Author}}{{end}}). Please help us cut down on flakes by [linking to](https://git.k8s.io/community/contributors/devel/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https://github.com/{{.Spec.Refs.Org}}/{{.Spec.Refs.Repo}}/issues?q=is:issue+is:open) when you hit one in your PR.'
    job_url_prefix: http://prow.cncf.ci/view/gcs/
    pod_pending_timeout: 60m
    # level=fatal msg="Error loading Prow config." component=checkconfig error="no default decoration image pull specs provided for plank"
    default_decoration_config:
      timeout: 7200000000000 # 2h
      grace_period: 15000000000 # 15s
      utility_images:
        sidecar: "gcr.io/k8s-prow/sidecar:v20190314-e8134a3"
        clonerefs: "gcr.io/k8s-prow/clonerefs:v20190314-e8134a3"
        initupload: "gcr.io/k8s-prow/initupload:v20190314-e8134a3"
        entrypoint: "gcr.io/k8s-prow/entrypoint:v20190314-e8134a3"
      gcs_configuration:
        bucket: "apisnoop"
        path_strategy: "legacy"
        default_org: "cncf"
        default_repo: "apisnoop"
      gcs_credentials_secret: "service-account"
#+END_SRC
*** periodics
#+BEGIN_SRC yaml
  periodics:
  - interval: 10m
    name: echo-test
    decorate: true
    spec:
      containers:
      - image: alpine
        command: ["/bin/date"]
#+END_SRC
*** postsubmits
#+BEGIN_SRC yaml
  postsubmits:
    cncf/apisnoop:
    - name: test-postsubmit
      decorate: true
      spec:
        containers:
        - image: alpine
          command: ["/bin/printenv"]
#+END_SRC
*** presubmits
#+BEGIN_SRC yaml
  presubmits:
    cncf/apisnoop:
    - name: test-presubmit
      decorate: true
      always_run: true
      skip_report: true
      spec:
        containers:
        - image: alpine
          command: ["/bin/printenv"]
#+END_SRC
** plugins.yaml
:PROPERTIES:
:header-args:yaml+: :tangle plugins.yaml
:END:
*** plugins_for_all_repos
[[*generate and replace the plugin config map][generate and replace the plugin config map]]   
#+BEGIN_SRC yaml :tangle no
  - approve
  - assign
  # - blockade # block pull requests from merging if they touch specific files
  - blunderbuss
  # - branchcleaner
  - cat
  # - cherry-pick-unapproved
  - cla
  # - config-updater # updates config/plugin.yaml for prow
  - docs-no-retest
  - dog
  - golint
  - heart
  - help
  - hold
  - label
  - lgtm
  - lifecycle
  # - milestone # needs a milestone group to allow setting milestone
  # - milestonestatus # needs a milestone group configured
  - override
  - owners-label
  # - pony
  # - project_manager
  # - project
  - release-note
  # - require-maching-label
  # - require-sig
  - shrug
  # - sigmention
  - size
  - skip
  # - slackevents
  # - stage
  - trigger
  - verify-owners
  - welcome
  - wip
  - yuks
#+END_SRC

THESE KEYS ==>>> ",bt"
RUN ME =>>>> [[update-plugins][update-plugins]]

*** plugins.yaml template
:PROPERTIES:
:header-args:yaml+: :tangle plugins.yaml
:END:

#+BEGIN_SRC yaml
  plugins:
    # cncf/apisnoop:
    # <<plugins_for_all_repos>>
    ii/apisnoop:
    <<plugins_for_all_repos>>
    ii/openfisca-aotearoa:
    <<plugins_for_all_repos>>
    ii/RapuTure:
    <<plugins_for_all_repos>>
#+END_SRC
i

*** project_config
**** Go Structs

[[file:~/test-infra/prow/plugins/config.go::type%20ProjectConfig%20struct]]

#+BEGIN_SRC go
// ProjectConfig contains the configuration options for the project plugin
type ProjectConfig struct {
	// Org level configs for github projects; key is org name
	Orgs map[string]ProjectOrgConfig `json:"project_org_configs,omitempty"`
}
#+END_SRC

#+BEGIN_SRC go
// ProjectOrgConfig holds the github project config for an entire org.
// This can be overridden by ProjectRepoConfig.
type ProjectOrgConfig struct {
	// ID of the github project maintainer team for a give project or org
	MaintainerTeamID int `json:"org_maintainers_team_id,omitempty"`
	// A map of project name to default column; an issue/PR will be added
	// to the default column if column name is not provided in the command
	ProjectColumnMap map[string]string `json:"org_default_column_map,omitempty"`
	// Repo level configs for github projects; key is repo name
	Repos map[string]ProjectRepoConfig `json:"project_repo_configs,omitempty"`
}
#+END_SRC

#+BEGIN_SRC go
// ProjectRepoConfig holds the github project config for a github project.
type ProjectRepoConfig struct {
	// ID of the github project maintainer team for a give project or org
	MaintainerTeamID int `json:"repo_maintainers_team_id,omitempty"`
	// A map of project name to default column; an issue/PR will be added
	// to the default column if column name is not provided in the command
	ProjectColumnMap map[string]string `json:"repo_default_column_map,omitempty"`
}
#+END_SRC
**** yaml

[[file:~/test-infra/prow/plugins.yaml::project_config:]]

We need to find the team_id.
https://developer.github.com/v3/teams/#list-teams
https://github.com/orgs/ii/teams/maintainer_team/members
 /orgs/:org/teams/:team_slug
curl /orgs/ii/teams/maintainer_team

#+BEGIN_SRC
curl --user hh:$GITHUB_TOKEN  -H "Accept:application/vnd.github.inertia-preview+json" https://api.github.com/orgs/ii/teams/maintainer_team | jq .id
#+END_SRC
#+BEGIN_SRC 
curl --user hh:$GITHUB_TOKEN  -H "Accept:application/vnd.github.inertia-preview+json" https://api.github.com/repos/ii/apisnoop/teams | jq .[0].id
#+END_SRC
#+BEGIN_SRC yaml
  project_config:
    project_org_configs:
      ii:
        org_maintainers_team_id: 3212487
        org_default_column_map:
          test-infra-dummy-testing-project-plugin:
            To do
          KEP Implementation Tracking:
            To do
        project_repo_configs:
          apisnoopcurl --user hh:$GITHUB_TOKEN  -H "Accept:application/vnd.github.inertia-preview+json" https://api.github.com/repos/ii/apisnoop/teams | jq .[0].id:
            repo_default_column_map:
              triage:
                triage
              need-attention:
                attention
#+END_SRC

#+BEGIN_SRC 
gcloud auth configure-docker

#+END_SRC


#+BEGIN_SRC yaml :noweb yes :tangle plugins.yaml
  # project_manager:
  #   org/repos:
  #     ii/apisnoop:
  #       projects:
  #         testProject:
  #           columns:
  #           - name: "triage"
  #             state: open
  #             org: ii
  #             labels:
  #             - area/conformance
 #+END_SRC
** configuration

*** install checkconfig
#+BEGIN_SRC shell
go get -u k8s.io/test-infra/prow/cmd/checkconfig
#+END_SRC

*** check the config files
#+BEGIN_SRC shell :results silent
  (
  checkconfig --plugin-config=plugins.yaml --config-path=config.yaml
  ) 2>&1
  echo -n $?
#+END_SRC
*** generate the plugin config map (dry run)
#+BEGIN_SRC shell :wrap "SRC yaml" :results silent
kubectl create configmap plugins \
  --from-file=plugins.yaml=plugins.yaml --dry-run -o yaml
#+END_SRC
*** generate and replace the plugin config map
EDIT ME =>>> [[plugins.yaml][plugins.yaml]]
#+BEGIN_SRC shell :noweb yes
kubectl create configmap plugins \
  --from-file=plugins.yaml=plugins.yaml --dry-run -o yaml \
  | kubectl replace configmap plugins -f -
#+END_SRC

#+RESULTS:
#+BEGIN_EXAMPLE :noeval t
configmap/plugins replaced
#+END_EXAMPLE

#+RESULTS: update-plugins
#+BEGIN_EXAMPLE :noeval t
configmap/plugins replaced
#+END_EXAMPLE

#+NAME: view-plugins
#+BEGIN_SRC shell :wrap "SRC yaml"
kubectl get configmap plugins -o json | jq -r '.data["plugins.yaml"]'
#+END_SRC

#+RESULTS: view-plugins
#+BEGIN_SRC yaml
# plugins.yaml template
# :PROPERTIES:
# :header-args:yaml+: :tangle plugins.yaml
# :END:


plugins:
  cncf/apisnoop:
  - approve
  - assign
  # - blockade # block pull requests from merging if they touch specific files
  - blunderbuss
  # - branchcleaner
  - cat
  # - cherry-pick-unapproved
  - cla
  # - config-updater # updates config/plugin.yaml for prow
  - docs-no-retest
  - dog
  - golint
  - heart
  - help
  - hold
  - label
  - lgtm
  - lifecycle
  # - milestone # needs a milestone group to allow setting milestone
  # - milestonestatus # needs a milestone group configured
  - override
  - owners-label
  # - pony
  # - project_manager
  - project
  - release-note
  # - require-maching-label
  # - require-sig
  - shrug
  # - sigmention
  - size
  - skip
  # - slackevents
  # - stage
  - trigger
  - verify-owners
  - welcome
  - wip
  - yuks
  ii/apisnoop:
  - approve
  - assign
  # - blockade # block pull requests from merging if they touch specific files
  - blunderbuss
  # - branchcleaner
  - cat
  # - cherry-pick-unapproved
  - cla
  # - config-updater # updates config/plugin.yaml for prow
  - docs-no-retest
  - dog
  - golint
  - heart
  - help
  - hold
  - label
  - lgtm
  - lifecycle
  # - milestone # needs a milestone group to allow setting milestone
  # - milestonestatus # needs a milestone group configured
  - override
  - owners-label
  # - pony
  # - project_manager
  - project
  - release-note
  # - require-maching-label
  # - require-sig
  - shrug
  # - sigmention
  - size
  - skip
  # - slackevents
  # - stage
  - trigger
  - verify-owners
  - welcome
  - wip
  - yuks
  ii/openfisca-aotearoa:
  - approve
  - assign
  # - blockade # block pull requests from merging if they touch specific files
  - blunderbuss
  # - branchcleaner
  - cat
  # - cherry-pick-unapproved
  - cla
  # - config-updater # updates config/plugin.yaml for prow
  - docs-no-retest
  - dog
  - golint
  - heart
  - help
  - hold
  - label
  - lgtm
  - lifecycle
  # - milestone # needs a milestone group to allow setting milestone
  # - milestonestatus # needs a milestone group configured
  - override
  - owners-label
  # - pony
  # - project_manager
  # - project
  - release-note
  # - require-maching-label
  # - require-sig
  - shrug
  # - sigmention
  - size
  - skip
  # - slackevents
  # - stage
  - trigger
  - verify-owners
  - welcome
  - wip
  - yuks
  ii/RapuTure:
  - approve
  - assign
  # - blockade # block pull requests from merging if they touch specific files
  - blunderbuss
  # - branchcleaner
  - cat
  # - cherry-pick-unapproved
  - cla
  # - config-updater # updates config/plugin.yaml for prow
  - docs-no-retest
  - dog
  - golint
  - heart
  - help
  - hold
  - label
  - lgtm
  - lifecycle
  # - milestone # needs a milestone group to allow setting milestone
  # - milestonestatus # needs a milestone group configured
  - override
  - owners-label
  # - pony
  # - project_manager
  - project
  - release-note
  # - require-maching-label
  # - require-sig
  - shrug
  # - sigmention
  - size
  - skip
  # - slackevents
  # - stage
  - trigger
  - verify-owners
  - welcome
  - wip
  - yuks

# yaml

# [[file:~/test-infra/prow/plugins.yaml::project_config:]]


project_config:
  project_org_configs:
    ii:
      # org_maintainers_team_id: ??
      org_default_column_map:
        test-infra-dummy-testing-project-plugin:
          To do
        KEP Implementation Tracking:
          To do
      project_repo_configs:
        apisnoop:
          repo_default_column_map:
            triage:
              triage
            need-attention:
              attention

# project_manager:
#   org/repos:
#     ii/apisnoop:
#       projects:
#         testProject:
#           columns:
#           - name: "triage"
#             state: open
#             org: ii
#             labels:
#             - area/conformance

#+END_SRC


#+NAME: generate the config configmap
#+BEGIN_SRC shell :wrap "SRC yaml" :results silent
kubectl create configmap config \
  --from-file=config.yaml=config.yaml --dry-run -o yaml
#+END_SRC

#+NAME: update-config
#+BEGIN_SRC shell
kubectl create configmap config \
  --from-file=config.yaml=config.yaml --dry-run -o yaml \
  | kubectl replace configmap config -f -
#+END_SRC

#+RESULTS: update-config
#+BEGIN_EXAMPLE :noeval t
configmap/config replaced
#+END_EXAMPLE

#+NAME: view-config
#+BEGIN_SRC shell :wrap "SRC yaml"
kubectl get configmap config -o json | jq -r '.data["config.yaml"]'
#+END_SRC

#+RESULTS: view-config
#+BEGIN_SRC yaml
# https://github.com/kubernetes/test-infra/issues/11729
#time="2019-03-18T07:06:59+13:00" level=fatal msg="Error loading Prow config."
# component=checkconfig error="no default decoration config provided for plank"
deck:
  spyglass:
    size_limit: 500e+6 # 500MB
    viewers:
      "started.json|finished.json": ["metadata"]
      "build-log.txt": ["buildlog"]
      "artifacts/junit.*\\.xml": ["junit"]
      # Remember to escape your '\' in yaml strings!
plank:
  job_url_template: 'https://job_url_template/'
#  job_url_template: '{{if .Spec.Refs}}{{if eq .Spec.Refs.Org "kubernetes-security"}}https://console.cloud.google.com/storage/browser/kubernetes-security-prow/{{else}}https://prow.k8s.io/view/gcs/kubernetes-jenkins/{{end}}{{else}}https://prow.k8s.io/view/gcs/kubernetes-jenkins/{{end}}{{if eq .Spec.Type "presubmit"}}pr-logs/pull{{else if eq .Spec.Type "batch"}}pr-logs/pull{{else}}logs{{end}}{{if .Spec.Refs}}{{if ne .Spec.Refs.Org ""}}{{if ne .Spec.Refs.Org "kubernetes"}}/{{if and (eq .Spec.Refs.Org "kubernetes-sigs") (ne .Spec.Refs.Repo "poseidon")}}sigs.k8s.io{{else}}{{.Spec.Refs.Org}}{{end}}_{{.Spec.Refs.Repo}}{{else if ne .Spec.Refs.Repo "kubernetes"}}/{{.Spec.Refs.Repo}}{{end}}{{end}}{{end}}{{if eq .Spec.Type "presubmit"}}/{{with index .Spec.Refs.Pulls 0}}{{.Number}}{{end}}{{else if eq .Spec.Type "batch"}}/batch{{end}}/{{.Spec.Job}}/{{.Status.BuildID}}/'
  report_template: '[Full PR test history](http://prow.cncf.ci/pr-history?org={{.Spec.Refs.Org}}&repo={{.Spec.Refs.Repo}}&pr={{with index .Spec.Refs.Pulls 0}}{{.Number}}{{end}}). [Your PR dashboard](https://gubernator.cncf.ci/pr/{{with index .Spec.Refs.Pulls 0}}{{.Author}}{{end}}). Please help us cut down on flakes by [linking to](https://git.k8s.io/community/contributors/devel/flaky-tests.md#filing-issues-for-flaky-tests) an [open issue](https://github.com/{{.Spec.Refs.Org}}/{{.Spec.Refs.Repo}}/issues?q=is:issue+is:open) when you hit one in your PR.'
  job_url_prefix: http://prow.cncf.ci/view/gcs/
  pod_pending_timeout: 60m
  # level=fatal msg="Error loading Prow config." component=checkconfig error="no default decoration image pull specs provided for plank"
  default_decoration_config:
    timeout: 7200000000000 # 2h
    grace_period: 15000000000 # 15s
    utility_images:
      sidecar: "gcr.io/k8s-prow/sidecar:v20190314-e8134a3"
      clonerefs: "gcr.io/k8s-prow/clonerefs:v20190314-e8134a3"
      initupload: "gcr.io/k8s-prow/initupload:v20190314-e8134a3"
      entrypoint: "gcr.io/k8s-prow/entrypoint:v20190314-e8134a3"
    gcs_configuration:
      bucket: "apisnoop"
      path_strategy: "legacy"
      default_org: "cncf"
      default_repo: "apisnoop"
    gcs_credentials_secret: "service-account"
periodics:
- interval: 10m
  name: echo-test
  decorate: true
  spec:
    containers:
    - image: alpine
      command: ["/bin/date"]
postsubmits:
  cncf/apisnoop:
  - name: test-postsubmit
    decorate: true
    spec:
      containers:
      - image: alpine
        command: ["/bin/printenv"]
presubmits:
  cncf/apisnoop:
  - name: test-presubmit
    decorate: true
    always_run: true
    skip_report: true
    spec:
      containers:
      - image: alpine
        command: ["/bin/printenv"]

#+END_SRC

* OWNERS
[[https://go.k8s.io/owners]]
[[https://github.com/kubernetes/community/blob/master/contributors/guide/owners.md]]

* debugging project plugin
  :PROPERTIES:
  :header-args:shell+: :dir ~/test-infra/
  :END:

** cards curls

#+BEGIN_SRC shell :wrap "SRC json"
  . ~/githubtoken
  curl --user hh:$GITHUB_TOKEN  -H "Accept:application/vnd.github.inertia-preview+json" \
  https://api.github.com/projects/columns/5090898/cards \
  | jq .[].id
  #https://api.github.com/projects/2501156/columns
#+END_SRC

#+RESULTS:
#+BEGIN_SRC json
20365896
20365897
#+END_SRC

#+BEGIN_SRC shell :wrap "SRC json"
  . ~/githubtoken
  curl --user hh:$GITHUB_TOKEN  -H "Accept:application/vnd.github.inertia-preview+json" \
  https://api.github.com/projects/columns/cards/20365897 \
  | jq -r .content_url
  #https://api.github.com/projects/2501156/columns
#+END_SRC

#+RESULTS:
#+BEGIN_SRC json
"https://api.github.com/repos/ii/apisnoop/issues/6"
#+END_SRC


** get secrets

*** hook secret

You need this for incoming secrets and to configure the github webhook or phony.

#+BEGIN_SRC shell
kubectl get secrets hmac-token -ojsonpath={.data.hmac} | base64 --decode > webhook-secret
#+END_SRC

*** oauth secret
#+BEGIN_SRC shell
kubectl get secrets oauth-token -ojsonpath={.data.oauth} | base64 --decode  > github-oauth
#+END_SRC

** hook config

#+BEGIN_SRC yaml :tangle ~/test-infra/hook-config.yaml
# I'm pretty sure we can use an empty config
#+END_SRC

** plugin config

#+BEGIN_SRC yaml :tangle ~/test-infra/hook-plugin.yaml
  plugins:
    ii/apisnoop:
    - label
    # - project_manager
    - project
    - trigger
  project_config:
    project_org_configs:
      ii:
        org_maintainers_team_id: 3212487
        org_default_column_map:
          test-infra-dummy-testing-project-plugin:
            To do
          KEP Implementation Tracking:
            To do
        project_repo_configs:
          apisnoop:
            repo_default_column_map:
              triage:
                triage
              need-attention:
                attention
#+END_SRC

** run hook locally
  
It runs on port all your IPs on port 8888.
Adding ii.cncf.ci:8888 as a hook should work.

#+BEGIN_SRC tmate
  cd ~/test-infra
  <<hook secret>>
  <<oauth secret>>
  go run prow/cmd/hook/main.go \
     --deck-url=https://prow.k8s.io \
     --config-path=hook-config.yaml \
     --plugin-config=hook-plugin.yaml \
     --hmac-secret-file=webhook-secret \
     --github-token-path=github-oauth \
     --dry-run=false
#+END_SRC
** run phony

#+BEGIN_SRC tmate
bazel run //prow/cmd/phony -- \
  --address=http://localhost:8888/hook \
  --hmac=$(cat webhook-secret) \
  --event=issue_comment \
  --payload=$(pwd)/comment.json
#+END_SRC

** event
*** comment_body

#+BEGIN_SRC json
  /project test_project2 need-attention
#+END_SRC

*** payload
#+BEGIN_SRC json :noweb yes :tangle ~/test-infra/comment.json
  {
    "action": "created",
    "issue": {
      "url": "https://api.github.com/repos/ii/apisnoop/issues/14",
      "repository_url": "https://api.github.com/repos/ii/apisnoop",
      "labels_url": "https://api.github.com/repos/ii/apisnoop/issues/14/labels{/name}",
      "comments_url": "https://api.github.com/repos/ii/apisnoop/issues/14/comments",
      "events_url": "https://api.github.com/repos/ii/apisnoop/issues/14/events",
      "html_url": "https://github.com/ii/apisnoop/issues/14",
      "id": 433492089,
      "node_id": "MDU6SXNzdWU0MzM0OTIwODk=",
      "number": 14,
      "title": "FOOBARBAZ",
      "user": {
        "login": "hh",
        "id": 31331,
        "node_id": "MDQ6VXNlcjMxMzMx",
        "avatar_url": "https://avatars2.githubusercontent.com/u/31331?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/hh",
        "html_url": "https://github.com/hh",
        "followers_url": "https://api.github.com/users/hh/followers",
        "following_url": "https://api.github.com/users/hh/following{/other_user}",
        "gists_url": "https://api.github.com/users/hh/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/hh/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/hh/subscriptions",
        "organizations_url": "https://api.github.com/users/hh/orgs",
        "repos_url": "https://api.github.com/users/hh/repos",
        "events_url": "https://api.github.com/users/hh/events{/privacy}",
        "received_events_url": "https://api.github.com/users/hh/received_events",
        "type": "User",
        "site_admin": false
      },
      "labels": [

      ],
      "state": "open",
      "locked": false,
      "assignee": null,
      "assignees": [

      ],
      "milestone": null,
      "comments": 171,
      "created_at": "2019-04-15T21:45:22Z",
      "updated_at": "2019-04-25T18:07:43Z",
      "closed_at": null,
      "author_association": "MEMBER",
      "body": "/woof"
    },
    "comment": {
      "url": "https://api.github.com/repos/ii/apisnoop/issues/comments/486779495",
      "html_url": "https://github.com/ii/apisnoop/issues/14#issuecomment-486779495",
      "issue_url": "https://api.github.com/repos/ii/apisnoop/issues/14",
      "id": 486779496,
      "node_id": "MDEyOklzc3VlQ29tbWVudDQ4Njc3OTQ5NQ==",
      "user": {
        "login": "hh",
        "id": 31331,
        "node_id": "MDQ6VXNlcjMxMzMx",
        "avatar_url": "https://avatars2.githubusercontent.com/u/31331?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/hh",
        "html_url": "https://github.com/hh",
        "followers_url": "https://api.github.com/users/hh/followers",
        "following_url": "https://api.github.com/users/hh/following{/other_user}",
        "gists_url": "https://api.github.com/users/hh/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/hh/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/hh/subscriptions",
        "organizations_url": "https://api.github.com/users/hh/orgs",
        "repos_url": "https://api.github.com/users/hh/repos",
        "events_url": "https://api.github.com/users/hh/events{/privacy}",
        "received_events_url": "https://api.github.com/users/hh/received_events",
        "type": "User",
        "site_admin": false
      },
      "created_at": "2019-04-25T18:07:43Z",
      "updated_at": "2019-04-25T18:07:43Z",
      "author_association": "MEMBER",
      "body": "<<comment_body>>"
    },
    "repository": {
      "id": 145496821,
      "node_id": "MDEwOlJlcG9zaXRvcnkxNDU0OTY4MjE=",
      "name": "apisnoop",
      "full_name": "ii/apisnoop",
      "private": false,
      "owner": {
        "login": "ii",
        "id": 30447,
        "node_id": "MDEyOk9yZ2FuaXphdGlvbjMwNDQ3",
        "avatar_url": "https://avatars2.githubusercontent.com/u/30447?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ii",
        "html_url": "https://github.com/ii",
        "followers_url": "https://api.github.com/users/ii/followers",
        "following_url": "https://api.github.com/users/ii/following{/other_user}",
        "gists_url": "https://api.github.com/users/ii/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/ii/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/ii/subscriptions",
        "organizations_url": "https://api.github.com/users/ii/orgs",
        "repos_url": "https://api.github.com/users/ii/repos",
        "events_url": "https://api.github.com/users/ii/events{/privacy}",
        "received_events_url": "https://api.github.com/users/ii/received_events",
        "type": "Organization",
        "site_admin": false
      },
      "html_url": "https://github.com/ii/apisnoop",
      "description": "Snooping on the Kubernetes OpenAPI communications",
      "fork": true,
      "url": "https://api.github.com/repos/ii/apisnoop",
      "forks_url": "https://api.github.com/repos/ii/apisnoop/forks",
      "keys_url": "https://api.github.com/repos/ii/apisnoop/keys{/key_id}",
      "collaborators_url": "https://api.github.com/repos/ii/apisnoop/collaborators{/collaborator}",
      "teams_url": "https://api.github.com/repos/ii/apisnoop/teams",
      "hooks_url": "https://api.github.com/repos/ii/apisnoop/hooks",
      "issue_events_url": "https://api.github.com/repos/ii/apisnoop/issues/events{/number}",
      "events_url": "https://api.github.com/repos/ii/apisnoop/events",
      "assignees_url": "https://api.github.com/repos/ii/apisnoop/assignees{/user}",
      "branches_url": "https://api.github.com/repos/ii/apisnoop/branches{/branch}",
      "tags_url": "https://api.github.com/repos/ii/apisnoop/tags",
      "blobs_url": "https://api.github.com/repos/ii/apisnoop/git/blobs{/sha}",
      "git_tags_url": "https://api.github.com/repos/ii/apisnoop/git/tags{/sha}",
      "git_refs_url": "https://api.github.com/repos/ii/apisnoop/git/refs{/sha}",
      "trees_url": "https://api.github.com/repos/ii/apisnoop/git/trees{/sha}",
      "statuses_url": "https://api.github.com/repos/ii/apisnoop/statuses/{sha}",
      "languages_url": "https://api.github.com/repos/ii/apisnoop/languages",
      "stargazers_url": "https://api.github.com/repos/ii/apisnoop/stargazers",
      "contributors_url": "https://api.github.com/repos/ii/apisnoop/contributors",
      "subscribers_url": "https://api.github.com/repos/ii/apisnoop/subscribers",
      "subscription_url": "https://api.github.com/repos/ii/apisnoop/subscription",
      "commits_url": "https://api.github.com/repos/ii/apisnoop/commits{/sha}",
      "git_commits_url": "https://api.github.com/repos/ii/apisnoop/git/commits{/sha}",
      "comments_url": "https://api.github.com/repos/ii/apisnoop/comments{/number}",
      "issue_comment_url": "https://api.github.com/repos/ii/apisnoop/issues/comments{/number}",
      "contents_url": "https://api.github.com/repos/ii/apisnoop/contents/{+path}",
      "compare_url": "https://api.github.com/repos/ii/apisnoop/compare/{base}...{head}",
      "merges_url": "https://api.github.com/repos/ii/apisnoop/merges",
      "archive_url": "https://api.github.com/repos/ii/apisnoop/{archive_format}{/ref}",
      "downloads_url": "https://api.github.com/repos/ii/apisnoop/downloads",
      "issues_url": "https://api.github.com/repos/ii/apisnoop/issues{/number}",
      "pulls_url": "https://api.github.com/repos/ii/apisnoop/pulls{/number}",
      "milestones_url": "https://api.github.com/repos/ii/apisnoop/milestones{/number}",
      "notifications_url": "https://api.github.com/repos/ii/apisnoop/notifications{?since,all,participating}",
      "labels_url": "https://api.github.com/repos/ii/apisnoop/labels{/name}",
      "releases_url": "https://api.github.com/repos/ii/apisnoop/releases{/id}",
      "deployments_url": "https://api.github.com/repos/ii/apisnoop/deployments",
      "created_at": "2018-08-21T02:40:29Z",
      "updated_at": "2019-03-19T19:24:31Z",
      "pushed_at": "2019-03-19T19:27:29Z",
      "git_url": "git://github.com/ii/apisnoop.git",
      "ssh_url": "git@github.com:ii/apisnoop.git",
      "clone_url": "https://github.com/ii/apisnoop.git",
      "svn_url": "https://github.com/ii/apisnoop",
      "homepage": null,
      "size": 18662,
      "stargazers_count": 0,
      "watchers_count": 0,
      "language": "CSS",
      "has_issues": true,
      "has_projects": true,
      "has_downloads": true,
      "has_wiki": true,
      "has_pages": false,
      "forks_count": 0,
      "mirror_url": null,
      "archived": false,
      "disabled": false,
      "open_issues_count": 10,
      "license": {
        "key": "mit",
        "name": "MIT License",
        "spdx_id": "MIT",
        "url": "https://api.github.com/licenses/mit",
        "node_id": "MDc6TGljZW5zZTEz"
      },
      "forks": 0,
      "open_issues": 10,
      "watchers": 0,
      "default_branch": "master"
    },
    "organization": {
      "login": "ii",
      "id": 30447,
      "node_id": "MDEyOk9yZ2FuaXphdGlvbjMwNDQ3",
      "url": "https://api.github.com/orgs/ii",
      "repos_url": "https://api.github.com/orgs/ii/repos",
      "events_url": "https://api.github.com/orgs/ii/events",
      "hooks_url": "https://api.github.com/orgs/ii/hooks",
      "issues_url": "https://api.github.com/orgs/ii/issues",
      "members_url": "https://api.github.com/orgs/ii/members{/member}",
      "public_members_url": "https://api.github.com/orgs/ii/public_members{/member}",
      "avatar_url": "https://avatars2.githubusercontent.com/u/30447?v=4",
      "description": "inclusively integrating the world"
    },
    "sender": {
      "login": "hh",
      "id": 31331,
      "node_id": "MDQ6VXNlcjMxMzMx",
      "avatar_url": "https://avatars2.githubusercontent.com/u/31331?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/hh",
      "html_url": "https://github.com/hh",
      "followers_url": "https://api.github.com/users/hh/followers",
      "following_url": "https://api.github.com/users/hh/following{/other_user}",
      "gists_url": "https://api.github.com/users/hh/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/hh/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/hh/subscriptions",
      "organizations_url": "https://api.github.com/users/hh/orgs",
      "repos_url": "https://api.github.com/users/hh/repos",
      "events_url": "https://api.github.com/users/hh/events{/privacy}",
      "received_events_url": "https://api.github.com/users/hh/received_events",
      "type": "User",
      "site_admin": false
    }
  }
#+END_SRC
** flow
*** restart

#+BEGIN_SRC tmate
  kubectl delete pods --namespace=default -l app=hook
#+END_SRC

*** logging

xargs -n 1 limits it to running one line at a time, so in parallel

#+BEGIN_SRC tmate
  kubectl get pods --namespace=default -l app=hook -o name \
  | sed s:pod/:: \
  | xargs -n 1 kubectl log -f
#+END_SRC

*** comment
Documentation for ~/project~ command:
https://prow.k8s.io/command-help

#+BEGIN_EXAMPLE
/project 0.5.0
/project 0.5.0 To do
/project clear 0.4.0
#+END_EXAMPLE

#+BEGIN_SRC note
/project test_project
#+END_SRC

** projects

1 test_project
2 test_project2
3 Projects Documentation

- [ ] list projects
- [ ] list project boards

* PR Status
[[https://github.com/kubernetes/test-infra/blob/master/prow/docs/pr_status_setup.md#how-to-setup-pr-status]]
*** github oauth app
**** secret/cookie

#+NAME: create github oauth cookie
#+BEGIN_SRC shell
  openssl rand -base64 64 > cookie
  kubectl create secret generic cookie \
    --from-file=secret=cookie
  # one liner attempt, that removes newlines to get a simple secret
  # kubectl create secret generic cookie \
  #   --from-literal=secret=$(openssl rand -base64 64 | tr -d "\n")
#+END_SRC

#+RESULTS: create github oauth cookie
#+BEGIN_EXAMPLE :noeval t
secret/cookie created
#+END_EXAMPLE

#+NAME: get secret/cookie
#+BEGIN_SRC shell :results silent
  kubectl get secret cookie  -o json | jq -r .data.secret | base64 -d
#+END_SRC

**** secret/github-oauth-config

#+NAME: github-oauth-config
#+BEGIN_SRC conf :tangle github-oauth-config
client_id: e4d9651867ae7a0f2d21
client_secret: XXXXX
redirect_url: http://prow.cncf.ci/github-login/redirect
final_redirect_url: http://prow.cncf.ci/pr
#+END_SRC

#+NAME: create secret/github-oauth-config
#+BEGIN_SRC shell
  kubectl create secret generic github-oauth-config \
    --from-file=secret=github-oauth-config
#+END_SRC

#+RESULTS: create secret/github-oauth-config
#+BEGIN_EXAMPLE :noeval t
secret/github-oauth-config created
#+END_EXAMPLE

#+NAME: gihub oauth-token
#+BEGIN_SRC shell :results silent
kubectl get secret github-oauth-config -o json | jq -r .data.secret | base64 -d
#+END_SRC
** Fix Oauth secret state

When visiting http://prow.cncf.ci/pr when we return from github/oauth to /github-login/redirect?code=X&state=Y we get the following error:

> 500 Internal server error Get secret state: empty string or cannot convert to string

Looking at the logs it might be related to the secrets:

#+BEGIN_EXAMPLE
kubectl log -f $(kubectl get pods --namespace=default -l app=deck -o name | tail -1 )
log is DEPRECATED and will be removed in a future version. Use logs instead.
time="2019-03-19T17:55:40Z" level=info msg="Spyglass registered viewer build-log-viewer with title Build Log." 
time="2019-03-19T17:55:40Z" level=info msg="Spyglass registered viewer junit-viewer with title JUnit." 
time="2019-03-19T17:55:40Z" level=info msg="Spyglass registered viewer metadata-viewer with title Metadata." 
{"client":"githuboauth","component":"deck","error":"empty string or cannot convert to string","level":"error","msg":"Error Get secret state.","time":"2019-03-19T17:56:48Z"}
{"client":"githuboauth","component":"deck","error":"empty string or cannot convert to string","level":"error","msg":"Error Get secret state.","time":"2019-03-19T17:56:53Z"}
^C
#+END_EXAMPLE

I ensured the secrets relating to oauth are in the correct place (as documented)

#+BEGIN_EXAMPLE
$ kubectl exec -ti $(kubectl get pods --namespace=default -l app=deck -o name | tail -1 | sed s:pod/::) /bin/sh
/app/prow/cmd/deck/app.binary.runfiles/test_infra # cat /etc/github/secret 
client_id: e4d9651867ae7a0f2d21
client_secret: 47XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXbb1
redirect_url: http://prow.cncf.ci/github-login/redirect
final_redirect_url: http://prow.cncf.ci/pr
/app/prow/cmd/deck/app.binary.runfiles/test_infra # cat /etc/cookie/secret 
wVXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXIX
sXXXXXXXXXXXXXXXXXXXXXXXXXXXXX==
#+END_EXAMPLE

Then I wanted to ensure it was looking in the locations I mounted them at.
(There are some command lines args for deck that enable it)

#+BEGIN_SRC yaml
      containers:
      - name: deck
        image: gcr.io/k8s-prow/deck:v20181109-1a84354
        args:
        - --tide-url=http://tide/
        - --hook-url=http://hook:8888/plugin-help
        - --oauth-url=/github-login
        - --github-oauth-config-file=/etc/github/secret
        - --cookie-secret=/etc/cookie/secret
        - --spyglass
#+END_SRC

After the Deployment recreated the pods with now args, I verified, but still got the same error:

#+BEGIN_EXAMPLE
/app/prow/cmd/deck/app.binary.runfiles/test_infra
# cat /proc/1/cmdline | sed s/--/\\n--/g
/app/prow/cmd/deck/app.binary
--tide-url=http://tide/
--hook-url=http://hook:8888/plugin-help
--oauth-url=/github-login
--github-oauth-config-file=/etc/github/secret
--cookie-secret=/etc/cookie/secret
--spyglass
#+END_EXAMPLE

* testgrid
https://k8s-testgrid.appspot.com/
https://k8s-testgrid.appspot.com/conformance-gce
[[https://k8s-testgrid.appspot.com/conformance-gce#GCE,%2520master%2520(dev)]]
Click on see these results in prow:

https://prow.k8s.io/job-history/kubernetes-jenkins/logs/ci-kubernetes-gce-conformance
 
https://github.com/kubernetes/test-infra/tree/master/testgrid#testgrid
https://github.com/kubernetes/test-infra/blob/master/testgrid/config.yaml


[[https://github.com/kubernetes/test-infra/blob/master/testgrid/config.yaml#L50][testgrid/config.yaml#testgroups]]
#+NAME: testgrid test_group ci-kubernetes-gce-conformance
#+BEGIN_SRC yaml
  #
  # Start testgroups
  #
  test_groups:
  # ... skipping lines ...
  # Prow hosted conformance tests
  - name: ci-kubernetes-gce-conformance
    gcs_prefix: kubernetes-jenkins/logs/ci-kubernetes-gce-conformance
    num_columns_recent: 3
    alert_stale_results_hours: 24
    num_failures_to_alert: 1
#+END_SRC

#+NAME: testgrid dashboard_tab: conformance-gce / GCE, master (dev)
#+BEGIN_SRC yaml
- name: conformance-gce
  dashboard_tab:
  - name: GCE, master (dev)
    description: Runs conformance tests using kubetest against latest kubernetes master CI build on GCE
    test_group_name: ci-kubernetes-gce-conformance
#+END_SRC

Looks like this runs four times a day.

* prow
Prow has many binaries and components.
Most are listed here:
https://github.com/kubernetes/test-infra/tree/master/prow/cmd

Job History: logs/ci-kubernetes-gce-conformance  
https://prow.k8s.io/job-history/kubernetes-jenkins/logs/ci-kubernetes-gce-conformance
https://prow.k8s.io/view/gcs/kubernetes-jenkins/logs/ci-kubernetes-gce-conformance/1114942954738290692
Gubernator, used to be used, but now we default to spyglass.
** jobs
[[https://github.com/kubernetes/test-infra/blob/master/config/jobs/kubernetes/sig-gcp/gce-conformance.yaml][config/jobs/kubernetes/sig-gcp/gce-conformance.yaml]]
#+BEGIN_SRC yaml
  periodics:
  - interval: 6h
    name: ci-kubernetes-gce-conformance
    labels:
      preset-service-account: "true"
      preset-k8s-ssh: "true"
    spec:
      containers:
      - args:
        - --timeout=220
        - --bare
        - --scenario=kubernetes_e2e
        - --
        - --extract=ci/latest
        - --gcp-master-image=gci
        - --gcp-node-image=gci
        - --gcp-zone=us-west1-b
        - --provider=gce
        - --test_args=--ginkgo.focus=\[Conformance\] --ginkgo.skip=Alpha|\[(Disruptive|Feature:[^\]]+|Flaky)\]
        - --timeout=200m
        image: gcr.io/k8s-testimages/kubekins-e2e:v20190329-811f7954b-master
#+END_SRC
** kubekins
   Wrapper to kubetest
[[https://github.com/kubernetes/test-infra/blob/master/images/kubekins-e2e/kops-e2e-runner.sh#L91][images/kubekins-e2e/kops-e2e-runner.sh#L91]]
** kubetest
   Used to build, deploy, and run k8s tests
   https://github.com/kubernetes/test-infra/tree/master/kubetest#kubetest
** deck uses spyglass to render/view artifacts
[[https://github.com/kubernetes/test-infra/blob/master/prow/config.yaml#L26][prow/config.yaml#L26]]
#+BEGIN_SRC yaml
deck:
  spyglass:
    size_limit: 500000000 # 500MB
    gcs_browser_prefix: https://gcsweb.k8s.io/gcs/
    testgrid_config: gs://k8s-testgrid/config
    testgrid_root: https://testgrid.k8s.io/
    viewers:
      "started.json|finished.json":
      - "metadata"
      "build-log.txt":
      - "buildlog"
      "artifacts/junit.*\\.xml":
      - "junit"
    announcement: "The old job viewer, Gubernator, has been deprecated in favour of this page, Spyglass.{{if .ArtifactPath}} For now, the old page is <a href='https://gubernator.k8s.io/build/{{.ArtifactPath}}'>still available</a>.{{end}} Please send feedback to sig-testing."
  tide_update_period: 1s
  hidden_repos:
  - kubernetes-security
  google_analytics: UA-82843984-5
#+END_SRC
** spyglass :: pluggable artifact viewer framework for Prow
   https://github.com/kubernetes/test-infra/tree/master/prow/spyglass#spyglass
   A general Spyglass query will proceed as follows:

- User provides a job source in the query (usually a job name and build ID).
- Spyglass finds all artifact names associated with the given job source.
- Spyglass builds a cache of which artifacts match which lenses via configured regular expressions.
- Lenses with matching artifacts are pre-rendered in order of descending priority.
- Spyglass then sends render requests to each registered lens with its matching artifacts.
- Each lens performs any necessary operations on the artifacts and produces a blob of HTML.
- Views (HTML) are inserted asynchronously as viewers return.
[[https://github.com/kubernetes/test-infra/blob/master/prow/cmd/deck/template/spyglass.html][prow/cmd/dock/template/spyglass.html]]
[[https://github.com/kubernetes/test-infra/tree/master/prow/spyglass#available-views][Available Views]]
*** Lenses :: set of functions that consume a list of artifacts and produces some HTML.
[[https://github.com/kubernetes/test-infra/tree/master/prow/spyglass#built-in-viewers][Built In Viewers]]
**** metadata
Clicking on more / less info pops down details.
**** junit
Shows Tests Skpipped and Passed!
**** Build Log
Link to raw build-log.txt
Default only shows lines with ERROR, but can show more.
*** Building our own Lense 
- [ ] Write Boiler Plate
- [ ] Implement
- [ ] Add to config
https://github.com/kubernetes/test-infra/tree/master/prow/spyglass#config
**** Debugging the prow / deck / spyclass instance

#+NAME: retrieve deck logs
#+BEGIN_SRC shell
  #kubectl logs --namespace=default -l app=deck
  kubectl get pods -l app=deck -o name | xargs -n 1 kubectl logs -f --namespace=default
#+END_SRC

#+NAME: invalid view names
#+BEGIN_EXAMPLE
{"component":"deck","duration":277252565,"level":"info","msg":"Listed 55 artifacts.","time":"2019-04-07T22:52:21Z"}
{"component":"deck","error":"invalid view name","level":"error","msg":"Could not find artifact viewer","time":"2019-04-07T22:52:21Z",
"viewName":"junit"}
{"component":"deck","error":"invalid view name","level":"error","msg":"Could not find artifact viewer","time":"2019-04-07T22:52:21Z",
"viewName":"buildlog"}
{"component":"deck","error":"invalid view name","level":"error","msg":"Could not find artifact viewer","time":"2019-04-07T22:52:21Z",
"viewName":"metadata"}
{"component":"deck","level":"info","msg":"job history link: /job-history/kubernetes-jenkins/logs/ci-kubernetes-gce-conformance","time":"2019-04-07T22:52:21Z"}
{"component":"deck","duration":"278.817799ms","level":"info","msg":"Rendered spyglass views.","source":"gcs/kubernetes-jenkins/logs/ci-kubernetes-gce-conformance/1114942954738290692","time":"2019-04-07T22:52:21Z"}
{"component":"deck","duration":"279.194765ms","endpoint":"/view/gcs/kubernetes-jenkins/logs/ci-kubernetes-gce-conformance/1114942954738290692","level":"info","msg":"Loading view completed.","source":"gcs/kubernetes-jenkins/logs/ci-kubernetes-gce-conformance/1114942954738290692","time":"2019-04-07T22:52:21Z"}
#+END_EXAMPLE
**** implement
    https://github.com/kubernetes/test-infra/tree/master/prow/spyglass#implementa

#+NAME: Lens.Interface
#+BEGIN_SRC go
type Lens interface {
	// Config returns the name, title, priority, and other information about your lens.
	Config() LensConfig
	// Header is used to inject content into the lens's <head>. It will only ever be called once per load.
	Header(artifacts []Artifact, resourceDir string) string
	// Body is used to generate the contents of the lens's <body>. It will initially be called with empty data, but
	// the lens front-end code may choose to re-render itself with custom data.
	Body(artifacts []Artifact, resourceDir string, data string) string
	// Callback is used for the viewer to exchange arbitrary data with the frontend. It is called with lens-specified
	// data, and returns data to be passed to the lens. JSON encoding is recommended in both directions.
	Callback(artifacts []Artifact, resourceDir string, data string) string
}
#+END_SRC

In the init method, call lenses.RegisterLens() with an instance of your implementation of the interface.
Spyglass should now be aware of your lens.

Additionally, some front-end TypeScript code can be provided.
Configure your BUILD.bazel to build it,
then emit a <script> tag with a relative reference to it in your Header() implementation.
See buildlog/BUILD.bazel for an example.

In your typescript code, a global spyglass object will be available, providing the following interface:

#+BEGIN_SRC js
export interface Spyglass {
  /**
   * Replaces the lens display with a new server-rendered page.
   * The returned promise will be resolved once the page has been updated.
   */
  updatePage(data: string): Promise<void>;
  /**
   * Requests that the server re-render the lens with the provided data, and
   * returns a promise that will resolve with that HTML as a string.
   *
   * This is equivalent to updatePage(), except that the displayed content is
   * not automatically changed.
   */
  requestPage(data: string): Promise<string>;
  /**
   * Sends a request to the server-side lens backend with the provided data, and
   * returns a promise that will resolve with the response as a string.
   */
  request(data: string): Promise<string>;
  /**
   * Inform Spyglass that the lens content has updated. This should be called whenever
   * the visible content changes, so Spyglass can ensure that all content is visible.
   */
  contentUpdated(): void;
}
#+END_SRC
** gcsweb
Allow anyone to browse GCS files / buckets?
https://gcsweb.k8s.io/gcs/kubernetes-jenkins/logs/ci-kubernetes-gce-conformance/1114942954738290692/
** build status on source.google.cloud.com
https://source.cloud.google.com/results/invocations/0b8cf342-71f9-4ede-a665-2ac712beb20a/targets/test/tests;group=Kubernetes%20e2e%20suite%20k8s.io%20LinuxOnly%20NodeConformance%20Conformance;test=KubeletManagedEtcHosts%20should%20test%20kubelet%20managed%20%2Fetc%2Fhosts%20file;row=17
* Developing spyglass
Dependencies - Bazel 0.23.0

#+BEGIN_SRC tmate
cd ~/test-infra/prow/cmd/deck
./runlocal
#+END_SRC

#+BEGIN_SRC shell
sed -i sXhttps://prow.k8s.ioXhttp://localhost:8080Xg ../../config.yaml
sed -i sXhttps://prow.k8s.ioXhttp://localhost:8080Xg ./localdata/*js

#+END_SRC
#+RESULTS:
#+BEGIN_EXAMPLE :noeval t
#+END_EXAMPLE

Visit http://localhost:8080

* Footnotes

#+PROPERTY: header-args:shell :results output code verbatim replace
#+PROPERTY: header-args:shell+ :exports both
#+PROPERTY: header-args:shell+ :wrap "EXAMPLE :noeval t"
#+PROPERTY: header-args:shell+ :eval no-export
#+PROPERTY: header-args:shell+ :noweb-ref (nth 4 (org-heading-components))
#+PROPERTY: header-args:tmate  :socket (symbol-value 'socket)
#+PROPERTY: header-args:tmate+ :session (concat (user-login-name) ":" (nth 4 (org-heading-components)))
#+PROPERTY: header-args:tmate+ :noweb yes
#+PROPERTY: header-args:json  :noweb yes
#+PROPERTY: header-args:json+ :noweb-ref (nth 4 (org-heading-components))
#+PROPERTY: header-args:yaml  :noweb yes
#+PROPERTY: header-args:yaml+ :comments org
#+PROPERTY: header-args:yaml+ :noweb-ref (nth 4 (org-heading-components))
#+REVEAL_ROOT: http://cdn.jsdelivr.net/reveal.js/3.0.0/
#+STARTUP: content
# Local Variables:
# eval: (set (make-local-variable 'org-file-dir) (file-name-directory buffer-file-name))
# eval: (set (make-local-variable 'user-buffer) (concat user-login-name "." (file-name-base buffer-file-name)))
# eval: (set (make-local-variable 'tmpdir) (make-temp-file (concat "/dev/shm/" user-buffer "-") t))
# eval: (set (make-local-variable 'socket) (concat "/tmp/" user-buffer ".iisocket"))
# eval: (set (make-local-variable 'select-enable-clipboard) t)
# eval: (set (make-local-variable 'select-enable-primary) t)
# eval: (set (make-local-variable 'start-tmate-command) (concat "tmate -S " socket " new-session -A -s " user-login-name " -n main \"tmate wait tmate-ready && tmate display -p '#{tmate_ssh}' | xclip -i -sel p -f | xclip -i -sel c; bash --login\""))
# eval: (xclip-mode 1)
# eval: (gui-select-text start-tmate-command)
# eval: (xclip-mode 1)
# org-babel-tmate-session-prefix: ""
# org-babel-tmate-default-window-name: "main"
# org-confirm-babel-evaluate: nil
# org-use-property-inheritance: t
# End:

