#+TITLE: Asn_pipeline_docker_file
Goal: Create docker image we can ultimately run in a prow job.
This document concerns itself with the docker file I will use to get an initial docker pipeline service working

* Outcomes definitions
k8s-infra-ii-sandbox:etl_staging.all_asn_company_outer_join
 A table that contains the following columns
 - asn
 - company_name
 - cidr_ip
 - start_ip
 - end_ip
 - name_yaml
 - redirectsToRegistry
 - redirectsToArtifacts
 - region
 - website
 - email
* Files to process data sources
 - Get ans from the yaml on k8s.io
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn_k8s_yaml.org
 - Get asn2company names from potaroo.net
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/etl_asn_company_table.org
 - Get asn metadata (email, website) from peeringdb
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/etl_asn_metadata_table.org
 - Get ans vendor (asn2iprange) from pyasn
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/etl_asn_vendor_table.org
* Other asn files out of scope here, but important to remember
 - Loading data from the prod logs to get clientip
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn-data.org
 - Mapping the ranges from the asn2iprange set to client_ip
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/match-ip-to-ip-range.org
* Concerns
 - I have never run the above 4 together, also part of the vendor creation process specifically
   converting string_ip to int uses bq to do the conversion, I do not want to over complicate the loading.

* Lets get to it

** Container image

Define the image
#+begin_src dockerfile :tangle ./Dockerfile
FROM postgres:12.2
RUN apt-get update && \
  apt-get install  -y --no-install-recommends \
  python3 \
  python3-dev \
  python3-pip \
  python3-wheel \
  python3-setuptools \
  jq \
  curl \
  git \
  gcc \
  libc6-dev
RUN pip3 install pyasn
#+end_src

Build the image
#+begin_src tmate :window asn-etl
docker build -t asn-etl-pipeline .
#+end_src

Test it out
#+begin_src tmate :window asn-etl
docker run -it --rm asn-etl-pipeline
#+end_src

* Discuss architecture with Caleb.
- build container image
  - img that has
    - PG
    - Python
      - BGP, PG, BQ
    - Script

- Job
  - based on image
  - runs script



* Next steps:
** Create container image we will use for the job
** Start converting org file into script we will be running on innit
*** For the first script I will get the asn_company lookup from potaroo:
https://github.com/ii/org/blob/main/research/asn-data-pipeline/etl_asn_company_table.org
*** For the second related step I will use pyasn:
https://github.com/ii/org/blob/main/research/asn-data-pipeline/etl_asn_vendor_table.org


* Shell script
** Pre-condition for shell
*** Gcloud
Log into gs cloud
#+BEGIN_SRC tmate :window prepare
gcloud auth login
#+END_SRC
Set default project
#+BEGIN_SRC tmate :window prepare
gcloud config set project k8s-infra-ii-sandbox
#+END_SRC
Checking my csv still exists
#+begin_src shell
gsutil ls -al gs://ii_bq_scratch_dump/ | grep peeringdb_company_asn
#+end_src
#+BEGIN_SRC python :dir (concat (getenv "HOME") "/foo")
## Import pyasn and csv
import pyasn
import csv

## Set file path
asnFile = "/home/ii/foo/asnNumbersOnly.txt"
## Open asnNumFile and read
asnNum = [line.rstrip() for line in open(asnFile, "r+")]

## assign our dat file connection string
asndb = pyasn.pyasn('ipasn_20140531_1.dat')
## Declare empty dictionary
destDict = {}
singleAsn = ""

## Loop through list of asns
for singleAsn in asnNum:
    ## Go look up the asn subnets (prefixes)
    subnets = asndb.get_as_prefixes(singleAsn)
    ## Add checking to make sure we have subnets
    ## TODO: insert asn with no routes so we know which failed without having to do a lookup
    if not subnets:
        print("This ASN has no subnets", singleAsn)
    else:
        ## Add subnets to our dictionaries with
        originAsnDict = {sbnets : singleAsn for sbnets in subnets}
        ## This is what lets us append each loop to the final destDict
        destDict.update(originAsnDict)

## Open handle to output file
resultsCsv = open("pyAsnOutput.csv", "w")
# write to csv
writer = csv.writer(resultsCsv)
for key, value in destDict.items():
    writer.writerow([key, value])

## winner winner chicken dinner
#+END_SRC
*** Python script we need for pyasn
** main shell
#+BEGIN_SRC shell :tangle "/tmp/main_etl_processor.sh")
#!/bin/bash
    ## D Lets set up our environment (this will be done in dockerfile)
    mkdir /tmp/foo
    ## This will pull a fresh copy, I prefer to use what we have in gs
    # curl -s  https://bgp.potaroo.net/cidr/autnums.html | sed -nre '/AS[0-9]/s/.*as=([^&]+)&.*">([^<]+)<\/a> ([^,]+), (.*)/"\1", "\3", "\4"/p'  | head
    # TODO: add if statement to do manual parsing if the gs file is not there
    gsutil cp gs://ii_bq_scratch_dump/potaroo_company_asn.csv  /tmp/potaroo.csv

    ## I want to import the above csv into pg
    ## Blocked by pg container
    
    ## Ok Caleb was going to create the docker imgage based on
    ## https://github.com/cncf/apisnoop/blob/main/apps/snoopdb/postgres/Dockerfile
    ## He is needed in xds
    ## I need to switch gears to get a container I can test in

    ## PYASN section
    ## D This moves to docker file when it is ready
    git clone https://github.com/hadiasghari/pyasn.git
    pip install pyasn

    ## pyasn installs its utils in ~/.local/bin/*
    ## Add pyasn utils to path (dockerfile?)
    export PATH="/home/ii/.local/bin/:$PATH"
    ## full list of RIB files on ftp://archive.routeviews.org//bgpdata/2021.05/RIBS/ 
    cd /tmp/foo
    pyasn_util_download.py --latest
    ## Convert rib file to .dat we can process
    pyasn_util_convert.py --single rib.latest.bz2 ipasn_latest.dat
    
    
#+end_src

#+RESULTS:
#+begin_example
#+end_example


#+begin_src shell
chmod +x /tmp/main_etl_processor.sh
. /tmp/main_etl_processor.sh
#+end_src

#+RESULTS:
#+begin_example
hi me
#+end_example
