#+TITLE: Asn_pipeline_docker_file
#+PROPERTY: header-args:sql-mode+ :comments none

Goal: Create docker image we can ultimately run in a prow job.
This document concerns itself with the docker file I will use to get an initial docker pipeline service working

* Outcomes definitions
k8s-infra-ii-sandbox:etl_staging.all_asn_company_outer_join
 A table that contains the following columns
 - asn
 - company_name
 - cidr_ip
 - start_ip
 - end_ip
 - name_yaml
 - redirectsToRegistry
 - redirectsToArtifacts
 - region
 - website
 - email
* Files to process data sources
 - Get ans from the yaml on k8s.io
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn_k8s_yaml.org
 - Get asn, datacenter from vendor json
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn_k8s_yaml.org
 - Get asn2company names from potaroo.net
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/etl_asn_company_table.org
 - Get asn metadata (email, website) from peeringdb
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/etl_asn_metadata_table.org
 - Get ans vendor (asn2iprange) from pyasn
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/etl_asn_vendor_table.org
* Other asn files out of scope here, but important to remember
 - Loading data from the prod logs to get clientip
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn-data.org
 - Mapping the ranges from the asn2iprange set to client_ip
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/match-ip-to-ip-range.org
* Concerns
 - I have never run the above 4 together, also part of the vendor creation process specifically
   converting string_ip to int uses bq to do the conversion, I do not want to over complicate the loading.

* Lets get to it

** Container image

Define the image
#+begin_src dockerfile :tangle ./Dockerfile :comments none
FROM golang:1.16.5-stretch as godeps
RUN go get -u github.com/mikefarah/yq/v4 && \
  test -f /go/bin/yq

FROM postgres:12.2
RUN apt-get update && \
  apt-get install -y curl && \
  echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main" \
    | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \
  curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - && \
  apt-get update && \
  apt-get install  -y --no-install-recommends \
  python3 \
  python3-dev \
  python3-pip \
  python3-wheel \
  python3-setuptools \
  jq \
  curl \
  git \
  gcc \
  libc6-dev \
  gettext-base \
  google-cloud-sdk && \
  rm -rf /var/lib/apt/lists/*
RUN pip3 install pyasn
WORKDIR /app
COPY --from=godeps /go/bin/yq /usr/local/bin/yq
COPY ./pg-init.d docker-entrypoint-initdb.d
COPY ./app .
#+end_src

Set gcloud auth creds file we can use in the container
#+begin_src tmate asn-etl
export GOOGLE_APPLICATION_CREDENTIALS="./beCarefulDelete.json"
#+end_src

#+BEGIN_SRC tmate :window asn-etl
export \
    PGUSER=postgres \
    PGPASSWORD=password
#+END_SRC

Build the image
#+begin_src tmate :window asn-etl
docker build -t asn-etl-pipeline .
#+end_src

* Discuss architecture with Caleb.
- build container image
  - img that has
    - PG
    - Python
      - BGP, PG, BQ
    - Script

- Job
  - based on image
  - runs script



* Next steps:
** Create container image we will use for the job
** Start converting org file into script we will be running on innit


* Shell script
** Pre-condition for shell
*** TODO
- I am going to allow application use for my gcloud creds on this box
- Set peeringdb_user, peeringdb_password
- Update peeringdb config to go to postgres db
- Make sure pg_USR/PW is set
- Make sql scripts to run, how do I invoke?
- Running directory?

*** Gcloud
Log into gs cloud
#+BEGIN_SRC tmate :window prepare
gcloud auth login
#+END_SRC
Set default project
#+BEGIN_SRC tmate :window prepare
gcloud config set project k8s-infra-ii-sandbox
#+END_SRC
Checking my csv still exists
#+begin_src shell
gsutil ls -al gs://ii_bq_scratch_dump/ | grep peeringdb_company_asn
#+end_src
I need to configure my application-default-credentials
#+BEGIN_SRC tmate :window prepare
gcloud auth application-default login
#+END_SRC
*** Set peeringdb user

*** Python script we need for pyasn
#+BEGIN_SRC python :dir  "./ii_pyasn.py")
## Import pyasn and csv
import pyasn
import csv

## Set file path
asnFile = "/home/ii/foo/asnNumbersOnly.txt"
## Open asnNumFile and read
asnNum = [line.rstrip() for line in open(asnFile, "r+")]
g
## assign our dat file connection string
asndb = pyasn.pyasn('ipasn_20140531_1.dat')
## Declare empty dictionary
destDict = {}
singleAsn = ""

## Loop through list of asns
for singleAsn in asnNum:
    ## Go look up the asn subnets (prefixes)
    subnets = asndb.get_as_prefixes(singleAsn)
    ## Add checking to make sure we have subnets
    ## TODO: insert asn with no routes so we know which failed without having to do a lookup
    if not subnets:
        print("This ASN has no subnets", singleAsn)
    else:
        ## Add subnets to our dictionaries with
        originAsnDict = {sbnets : singleAsn for sbnets in subnets}
        ## This is what lets us append each loop to the final destDict
        destDict.update(originAsnDict)

## Open handle to output file
resultsCsv = open("pyAsnOutput.csv", "w")
# write to csv
writer = csv.writer(resultsCsv)
for key, value in destDict.items():
    writer.writerow([key, value])

## winner winner chicken dinner
#+end_src

*** Things we need in the docker file
**** pyasn:
-    git clone https://github.com/hadiasghari/pyasn.git
-    pip install pyasn
**** Peeringdb:
- Clone https://git.2e8.dk/peeringdb-simplesync (git clone https://git.2e8.dk/peeringdb-simplesync)
**** Set pg-sql creds so peeringdb can load csv into pg without needing to log in
pip install psycopg2-binary
**** Where do we run the peeringdb sync.py?
For now I will accomodate it in the script
*** Python config we need to set for peeringdb connections
#+BEGIN_SRC python :tangle "/tmp/config.py")
from requests.auth import HTTPBasicAuth
import os

host=os.environ['SHARINGIO_PAIR_LOAD_BALANCER_IP']
user=os.environ['PEERINGDB_USER']
password=os.environ['PEERINGDB_PASSWORD']

def get_config():
    return {
        'db_conn_str': 'dbname=peeringdb host=%s user=postgres password=password' % host,
        'db_schema': 'peeringdb',
        'auth': HTTPBasicAuth(user, password)
    }
#+END_SRC
*** Set the peeringdb creds
- set PEERINGDB_USER
- set PEERINGDB_PASSWORD

** Setting up an ServiceAccount in GCP for the pipeline
Create the ServiceAccount
#+begin_src shell :results silent
gcloud iam service-accounts create asn-etl \
    --display-name="asn-etl" \
    --description="A Service Account used for ETL with ASN data"
#+end_src

Assign the role to the ServiceAccount
#+begin_src shell :results silent
ROLES=(
    roles/bigquery.user
    roles/bigquery.dataEditor
)

for ROLE in ${ROLES[*]}; do
  gcloud projects add-iam-policy-binding k8s-infra-ii-sandbox \
    --member="serviceAccount:asn-etl@k8s-infra-ii-sandbox.iam.gserviceaccount.com" \
    --role="${ROLE}"
done
#+end_src

** Local testing

Generate a key file for ServiceAccount auth
#+begin_src shell :results silent
gcloud iam service-accounts keys create /tmp/asn-etl-pipeline-gcp-sa.json --iam-account=asn-etl@k8s-infra-ii-sandbox.iam.gserviceaccount.com
#+end_src

Change key permissions
#+begin_src shell :results silent
sudo chown 999 /tmp/asn-etl-pipeline-gcp-sa.json
#+end_src

Test it out
#+begin_src tmate :window asn-etl
TMP_DIR_ETL=$(mktemp -d)
sudo chmod 0777 "${TMP_DIR_ETL}"
docker run \
    -it \
    --rm \
    -e POSTGRES_PASSWORD="password" \
    -e GOOGLE_APPLICATION_CREDENTIALS=/tmp/asn-etl-pipeline-gcp-sa.json \
    -e GCP_PROJECT=k8s-infra-ii-sandbox \
    -e GCP_SERVICEACCOUNT=asn-etl@k8s-infra-ii-sandbox.iam.gserviceaccount.com \
    -e GCP_BIGQUERY_DATASET=etl_script_generated_set \
    -v /tmp/asn-etl-pipeline-gcp-sa.json:/tmp/asn-etl-pipeline-gcp-sa.json \
    -v "${PWD}/pg-init.d:/docker-entrypoint-initdb.d" \
    -v "${TMP_DIR_ETL}:/tmp" \
    asn-etl-pipeline
echo "${TMP_DIR_ETL}"
#+end_src

** Postgres init files

Given PyASN data, query the ASN data from the resulting /.dat/ file
#+begin_src python :tangle ./app/ip-from-pyasn.py :comments none
## Import pyasn and csv
import pyasn
import csv
import sys

## Set file path
asnFile = sys.argv[1]
asnDat = sys.argv[2]
pyAsnOutput = sys.argv[3]
## Open asnNumFile and read
asnNum = [line.rstrip() for line in open(asnFile, "r+")]

## assign our dat file connection string
asndb = pyasn.pyasn(asnDat)
## Declare empty dictionary
destDict = {}
singleAsn = ""

missingSubnets = []
## Loop through list of asns
for singleAsn in asnNum:
    ## Go look up the asn subnets (prefixes)
    subnets = asndb.get_as_prefixes(singleAsn)
    ## Add checking to make sure we have subnets
    ## TODO: insert asn with no routes so we know which failed without having to do a lookup
    if subnets:
        ## Add subnets to our dictionaries with
        originAsnDict = {sbnets : singleAsn for sbnets in subnets}
        ## This is what lets us append each loop to the final destDict
        destDict.update(originAsnDict)

if len(missingSubnets) > 0:
    print("Subnets missing from ASNs: ", missingSubnets)

## Open handle to output file
resultsCsv = open(pyAsnOutput, "w")
# write to csv
writer = csv.writer(resultsCsv)
for key, value in destDict.items():
    writer.writerow([key, value])

## winner winner chicken dinner
#+end_src

#+BEGIN_SRC shell :tangle ./pg-init.d/00-get-dependencies.sh
#!/bin/bash
set -x

gcloud auth activate-service-account "${GCP_SERVICEACCOUNT}" --key-file="${GOOGLE_APPLICATION_CREDENTIALS}"
## GET ASN_COMAPNY section
## using https://github.com/ii/org/blob/main/research/asn-data-pipeline/etl_asn_company_table.org
## This will pull a fresh copy, I prefer to use what we have in gs
# curl -s  https://bgp.potaroo.net/cidr/autnums.html | sed -nre '/AS[0-9]/s/.*as=([^&]+)&.*">([^<]+)<\/a> ([^,]+), (.*)/"\1", "\3", "\4"/p'  | head
# TODO: add if statement to do manual parsing if the gs file is not there

if [ ! -f "/tmp/potaroo_data.csv" ]; then
    gsutil cp gs://ii_bq_scratch_dump/potaroo_company_asn.csv  /tmp/potaroo_data.csv
fi

# Strip data to only return ASN numbers
cat /tmp/potaroo_data.csv | cut -d ',' -f1 | sed 's/"//' | sed 's/"//'| cut -d 'S' -f2 | tail +2 >> /tmp/potaroo_asn.txt

## GET PYASN section
## using https://github.com/ii/org/blob/main/research/asn-data-pipeline/etl_asn_vendor_table.org

## pyasn installs its utils in ~/.local/bin/*
## Add pyasn utils to path (dockerfile?)
## full list of RIB files on ftp://archive.routeviews.org//bgpdata/2021.05/RIBS/
cd /tmp
if [ ! -f "rib.latest.bz2" ]; then
  pyasn_util_download.py --latest
  mv rib.*.*.bz2 rib.latest.bz2
fi
## Convert rib file to .dat we can process
if [ ! -f "ipasn_latest.dat" ]; then
  pyasn_util_convert.py --single rib.latest.bz2 ipasn_latest.dat
fi
## Run the py script we are including in the docker image
python3 /app/ip-from-pyasn.py /tmp/potaroo_asn.txt ipasn_latest.dat /tmp/pyAsnOutput.csv
## This will output pyasnOutput.csv
#+END_SRC

SQL for migrating the database
#+begin_src sql-mode :tangle ./pg-init.d/01-migrate-schemas.sql
begin;

create table company_asn (
  asn varchar,
  name varchar
);
create table pyasn_ip_asn (
  ip cidr,
  asn int
);
create table asnproc (
  asn bigint not null primary key
);

create schema peeringdb;

create table peeringdb.org (
	id int not null,
	status text not null,
	data jsonb not null,
	created timestamptz not null,
	updated timestamptz not null,
	deleted timestamptz,
	primary key (id)
);

create table peeringdb.net (
	id int not null,
	org_id int not null,
	asn bigint not null,
	status text not null,
	data jsonb not null,
	created timestamptz not null,
	updated timestamptz not null,
	deleted timestamptz,
	primary key (id)
);

create table peeringdb.ix (
	id int not null,
	org_id int not null,
	status text not null,
	data jsonb not null,
	created timestamptz not null,
	updated timestamptz not null,
	deleted timestamptz,
	primary key (id)
);

create table peeringdb.fac (
	id int not null,
	org_id int not null,
	status text not null,
	data jsonb not null,
	created timestamptz not null,
	updated timestamptz not null,
	deleted timestamptz,
	primary key (id)
);

create table peeringdb.poc (
	id int not null,
	net_id int not null,
	status text not null,
	data jsonb not null,
	created timestamptz not null,
	updated timestamptz not null,
	deleted timestamptz,
	primary key (id)
);

create table peeringdb.ixlan (
	id int not null,
	ix_id int not null,
	status text not null,
	data jsonb not null,
	created timestamptz not null,
	updated timestamptz not null,
	deleted timestamptz,
	primary key (id)
);

create table peeringdb.ixpfx (
	id int not null,
	ixlan_id int not null,
	status text not null,
	data jsonb not null,
	created timestamptz not null,
	updated timestamptz not null,
	deleted timestamptz,
	primary key (id)
);

create table peeringdb.ixfac (
	id int not null,
	ix_id int not null,
	fac_id int not null,
	status text not null,
	data jsonb not null,
	created timestamptz not null,
	updated timestamptz not null,
	deleted timestamptz,
	primary key (id)
);

create table peeringdb.netfac (
	id int not null,
	net_id int not null,
	fac_id int not null,
	status text not null,
	data jsonb not null,
	created timestamptz not null,
	updated timestamptz not null,
	deleted timestamptz,
	primary key (id)
);

create table peeringdb.netixlan (
	id int not null,
	net_id int not null,
	ix_id int not null,
	ixlan_id int not null,
	status text not null,
	data jsonb not null,
	created timestamptz not null,
	updated timestamptz not null,
	deleted timestamptz,
	primary key (id)
);

commit;
#+end_src

#+begin_src sql-mode :tangle ./pg-init.d/02-load-pyasn-output.sql
-- Use the above output to create the pyasn_ip_asn table below
-- placeholder sql

COPY company_asn from '/tmp/potaroo_data.csv' DELIMITER ',' CSV;
COPY pyasn_ip_asn from '/tmp/pyAsnOutput.csv' DELIMITER ',' CSV;

-- Split subnet into start and end
select asn as asn,
ip as ip,
host(network(ip)::inet) as ip_start,
host(broadcast(ip)::inet) as ip_end
into table pyasn_ip_asn_extended
from pyasn_ip_asn;

-- Copy the results to cs
copy (select * from pyasn_ip_asn_extended) to '/tmp/pyasn_expanded_ipv4.csv' csv header;
#+end_src

Query for loading extended IP ASN ranges into BigQuery
#+begin_src sql-mode :tangle ./app/ext-ip-asn.sql
SELECT
    asn as asn,
    ip as cidr_ip,
    ip_start as start_ip,
    ip_end as end_ip,
    NET.IPV4_TO_INT64(NET.IP_FROM_STRING(ip_start)) AS start_ip_int,
    NET.IPV4_TO_INT64(NET.IP_FROM_STRING(ip_end)) AS end_ip_int
    FROM `k8s-infra-ii-sandbox.${GCP_BIGQUERY_DATASET}.pyasn_ip_asn_extended`
    WHERE regexp_contains(ip_start, r"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}");
#+end_src

#+begin_src shell :tangle ./pg-init.d/03-load-into-a-bigquery-dataset.sh

cat << EOF > $HOME/.bigqueryrc
credential_file = ${GOOGLE_APPLICATION_CREDENTIALS}
project_id = ${GCP_PROJECT}
EOF

gcloud config set project "${GCP_PROJECT}"

## Load csv to bq
bq load --autodetect "${GCP_BIGQUERY_DATASET}.pyasn_ip_asn_extended" /tmp/pyasn_expanded_ipv4.csv

## Lets go convert the beginning and end into ints
cat /app/ext-ip-asn.sql | envsubst | bq query --nouse_legacy_sql --replace --destination_table "${GCP_BIGQUERY_DATASET}.vendor"

mkdir -p /tmp/vendor

VENDORS=(
    microsoft
    google
    amazon
    alibabagroup
    baidu
    digitalocean
    equinixmetal
    huawei
    tencentcloud
)
## This should be the end of pyasn section, we have results table that covers start_ip/end_ip from fs our requirements
## GET k8s asn yaml using:
## https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn_k8s_yaml.org
## Lets create csv's to import
for VENDOR in ${VENDORS[*]}; do
  curl -s "https://raw.githubusercontent.com/kubernetes/k8s.io/main/registry.k8s.io/infra/meta/asns/${VENDOR}.yaml" \
      | yq e . -j - \
      | jq -r '.name as $name | .redirectsTo.registry as $redirectsToRegistry | .redirectsTo.artifacts as $redirectsToArtifacts | .asns[] | [.,$name, $redirectsToRegistry, $redirectsToArtifacts] | @csv' \
        > "/tmp/vendor/${VENDOR}_yaml.csv"
  bq load --autodetect "${GCP_BIGQUERY_DATASET}.vendor_json" "/tmp/vendor/${VENDOR}_yaml.csv"
done

ASN_VENDORS=(
    amazon
    google
    microsoft
)

## GET Vendor YAML
## https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn_k8s_yaml.org
curl "https://download.microsoft.com/download/7/1/D/71D86715-5596-4529-9B13-DA13A5DE5B63/ServiceTags_Public_$(date --date=yesterday +%Y%m%d).json" \
    | jq -r '.values[] | .properties.platform as $service | .properties.region as $region | .properties.addressPrefixes[] | [., $service, $region] | @csv' \
      > /tmp/vendor/microsoft_subnet_region.csv
curl 'https://www.gstatic.com/ipranges/cloud.json' \
    | jq -r '.prefixes[] | [.ipv4Prefix, .service, .scope] | @csv' \
      > /tmp/vendor/google_raw_subnet_region.csv
curl 'https://ip-ranges.amazonaws.com/ip-ranges.json' \
    | jq -r '.prefixes[] | [.ip_prefix, .service, .region] | @csv' \
      > /tmp/vendor/amazon_raw_subnet_region.csv

## Load all the csv
for VENDOR in ${ASN_VENDORS[*]}; do
  bq load --autodetect "${GCP_BIGQUERY_DATASET}.${VENDOR}_raw_subnet_region" "/tmp/vendor/${VENDOR}_raw_subnet_region.csv"
done

mkdir -p /tmp/peeringdb-tables
PEERINGDB_TABLES=(
    net
    poc
)
for PEERINGDB_TABLE in ${PEERINGDB_TABLES}; do
    curl -sG "https://www.peeringdb.com/api/${PEERINGDB_TABLE}" | jq '.data' > "/tmp/peeringdb-tables/${PEERINGDB_TABLE}.json"
done

# /tmp/potaroo_asn.txt

## placeholder for sql we will need to import asn_only from
#+end_src

#+begin_src sql-mode :tangle ./pg-init.d/04-load-asn-data.sql
copy asnproc from '/tmp/potaroo_asn.txt';
-- Placeholder sql for joining peeringdb to produce output with email, website
copy (
  select distinct asn.asn,
  (net.data ->> 'name') as "name",
  (net.data ->> 'website') as "website",
  (poc.data ->> 'email') as email
  into asn_name_web_email
  from asnproc asn
  left join peeringdb.net net on (net.asn = asn.asn)
  left join peeringdb.poc poc on ((poc.data ->> 'name') = (net.data ->> 'name'))
-- where (net.data ->>'website') is not null
-- where (poc.data ->> 'email') is not null
  order by email asc) to '/tmp/peeringdb_metadata.csv' csv header;;
#+end_src

#+begin_src shell
## Load output to bq
bq load --autodetect k8s_artifacts_dataset_bb_test.amazon_raw_subnet_region /tmp/vendor/amazon_raw_subnet_region.csv
#+end_src
