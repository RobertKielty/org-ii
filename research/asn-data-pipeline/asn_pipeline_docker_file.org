#+TITLE: Asn_pipeline_docker_file
Goal: Create docker image we can ultimately run in a prow job.
This document concerns itself with the docker file I will use to get an initial docker pipeline service working

* Outcomes definitions
k8s-infra-ii-sandbox:etl_staging.all_asn_company_outer_join
 A table that contains the following columns
 - asn
 - company_name
 - cidr_ip
 - start_ip
 - end_ip
 - name_yaml
 - redirectsToRegistry
 - redirectsToArtifacts
 - region
 - website
 - email
* Files to process data sources
 - Get ans from the yaml on k8s.io
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn_k8s_yaml.org
 - Get asn, datacenter from vendor json
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn_k8s_yaml.org
 - Get asn2company names from potaroo.net
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/etl_asn_company_table.org
 - Get asn metadata (email, website) from peeringdb
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/etl_asn_metadata_table.org
 - Get ans vendor (asn2iprange) from pyasn
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/etl_asn_vendor_table.org
* Other asn files out of scope here, but important to remember
 - Loading data from the prod logs to get clientip
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn-data.org
 - Mapping the ranges from the asn2iprange set to client_ip
   https://github.com/ii/org/blob/main/research/asn-data-pipeline/match-ip-to-ip-range.org
* Concerns
 - I have never run the above 4 together, also part of the vendor creation process specifically
   converting string_ip to int uses bq to do the conversion, I do not want to over complicate the loading.

* Lets get to it

** Container image

Define the image
#+begin_src dockerfile :tangle ./Dockerfile
FROM postgres:12.2
RUN apt-get update && \
  apt-get install -y curl && \
  echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main" \
    | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \
  curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - && \
  apt-get update && \
  apt-get install  -y --no-install-recommends \
  python3 \
  python3-dev \
  python3-pip \
  python3-wheel \
  python3-setuptools \
  jq \
  curl \
  git \
  gcc \
  libc6-dev \
  google-cloud-sdk && \
  rm -rf /var/lib/apt/lists/*
RUN pip3 install pyasn
COPY ./main_etl_processor.sh docker-entrypoint-initdb.d/
#+end_src

Build the image
#+begin_src tmate :window asn-etl
docker build -t asn-etl-pipeline .
#+end_src

Test it out
#+begin_src tmate :window asn-etl
docker run -it --rm asn-etl-pipeline
#+end_src

* Discuss architecture with Caleb.
- build container image
  - img that has
    - PG
    - Python
      - BGP, PG, BQ
    - Script

- Job
  - based on image
  - runs script



* Next steps:
** Create container image we will use for the job
** Start converting org file into script we will be running on innit
*** For the first script I will get the asn_company lookup from potaroo:
https://github.com/ii/org/blob/main/research/asn-data-pipeline/etl_asn_company_table.org
*** For the second related step I will use pyasn:
https://github.com/ii/org/blob/main/research/asn-data-pipeline/etl_asn_vendor_table.org


* Shell script
** Pre-condition for shell
*** Gcloud
Log into gs cloud
#+BEGIN_SRC tmate :window prepare
gcloud auth login
#+END_SRC
Set default project
#+BEGIN_SRC tmate :window prepare
gcloud config set project k8s-infra-ii-sandbox
#+END_SRC
Checking my csv still exists
#+begin_src shell
gsutil ls -al gs://ii_bq_scratch_dump/ | grep peeringdb_company_asn
#+end_src

*** Python script we need for pyasn
#+BEGIN_SRC python :dir  "./ii_pyasn.py")
## Import pyasn and csv
import pyasn
import csv

## Set file path
asnFile = "/home/ii/foo/asnNumbersOnly.txt"
## Open asnNumFile and read
asnNum = [line.rstrip() for line in open(asnFile, "r+")]

## assign our dat file connection string
asndb = pyasn.pyasn('ipasn_20140531_1.dat')
## Declare empty dictionary
destDict = {}
singleAsn = ""

## Loop through list of asns
for singleAsn in asnNum:
    ## Go look up the asn subnets (prefixes)
    subnets = asndb.get_as_prefixes(singleAsn)
    ## Add checking to make sure we have subnets
    ## TODO: insert asn with no routes so we know which failed without having to do a lookup
    if not subnets:
        print("This ASN has no subnets", singleAsn)
    else:
        ## Add subnets to our dictionaries with
        originAsnDict = {sbnets : singleAsn for sbnets in subnets}
        ## This is what lets us append each loop to the final destDict
        destDict.update(originAsnDict)

## Open handle to output file
resultsCsv = open("pyAsnOutput.csv", "w")
# write to csv
writer = csv.writer(resultsCsv)
for key, value in destDict.items():
    writer.writerow([key, value])

## winner winner chicken dinner
#+end_src

*** Things we need in the docker file
**** pyasn:
-    git clone https://github.com/hadiasghari/pyasn.git
-    pip install pyasn
**** Peeringdb:
- Clone https://git.2e8.dk/peeringdb-simplesync (git clone https://git.2e8.dk/peeringdb-simplesync)
**** Set pg-sql creds so peeringdb can load csv into pg without needing to log in
pip install psycopg2-binary
**** Where do we run the peeringdb sync.py?
For now I will accomodate it in the script
*** Python config we need to set for peeringdb connections
#+BEGIN_SRC python :tangle (concat (getenv "HOME") "/peeringdb-simplesync/config.py")
from requests.auth import HTTPBasicAuth
import os

host=os.environ['SHARINGIO_PAIR_LOAD_BALANCER_IP']
user=os.environ['PEERINGDB_USER']
password=os.environ['PEERINGDB_PASSWORD']

def get_config():
    return {
        'db_conn_str': 'dbname=peeringdb host=%s user=postgres password=password' % host,
        'db_schema': 'peeringdb',
        'auth': HTTPBasicAuth(user, password)
    }
#+END_SRC
*** Set the peeringdb creds
- set PEERINGDB_USER
- set PEERINGDB_PASSWORD





** main shell
#+BEGIN_SRC shell :tangle ./main_etl_processor.sh
#!/bin/bash
    ## Lets set up our environment (this will be done in dockerfile)
    mkdir /tmp/foo

    ## GET ASN_COMAPNY section
    ## using https://github.com/ii/org/blob/main/research/asn-data-pipeline/etl_asn_company_table.org
    ## This will pull a fresh copy, I prefer to use what we have in gs
    # curl -s  https://bgp.potaroo.net/cidr/autnums.html | sed -nre '/AS[0-9]/s/.*as=([^&]+)&.*">([^<]+)<\/a> ([^,]+), (.*)/"\1", "\3", "\4"/p'  | head
    # TODO: add if statement to do manual parsing if the gs file is not there
    gsutil cp gs://ii_bq_scratch_dump/potaroo_company_asn.csv  /tmp/potaroo.csv

    ## I want to import the above csv into pg
    ## Blocked by pg container
    ## placeholder sql
   --create table company_asn  (asn varchar, name varchar);
   \COPY company_asn from '/home/ii/autonums/asn_company_results.csv' DELIMITER ',' CSV;


    ## GET PYASN section
    ## using https://github.com/ii/org/blob/main/research/asn-data-pipeline/etl_asn_vendor_table.org

    ## pyasn installs its utils in ~/.local/bin/*
    ## Add pyasn utils to path (dockerfile?)
    export PATH="/home/ii/.local/bin/:$PATH"
    ## full list of RIB files on ftp://archive.routeviews.org//bgpdata/2021.05/RIBS/
    cd /tmp/foo
    pyasn_util_download.py --latest
    ## Convert rib file to .dat we can process
    pyasn_util_convert.py --single rib.latest.bz2 ipasn_latest.dat
    ## Run the py script we are including in the docker image
    python ./ii-pyasn.py

    ## Load csv into pg
    ## placeholder sql
    create table pyasn_ip_asn  (ip cidr, asn int);
    \COPY pyasn_ip_asn from '/home/ii/foo/pyAsnOutput.csv' DELIMITER ',' CSV;
    ## Split subnet into start and end
      select asn as asn,
      ip as ip,
      host(network(ip)::inet) as ip_start,
      host(broadcast(ip)::inet) as ip_end
      into table pyasn_ip_asn_extended
      from pyasn_ip_asn;

     ## Copy the results to cs
     \copy (select * from pyasn_ip_asn_extended) to '/tmp/pyasn_expanded_ipv4.csv' csv header;
     ## Load csv to bq
     bq load --autodetect k8s_artifacts_dataset_bb_test.pyasn_ip_asn_extended /tmp/pyasn_expanded_ipv4.csv
     ## Lets go convert the beginning and end into ints
       bq query --nouse_legacy_sql \
       '
       SELECT
         asn as asn,
         ip as cidr_ip,
         ip_start as start_ip,
         ip_end as end_ip,
         NET.IPV4_TO_INT64(NET.IP_FROM_STRING(ip_start)) AS start_ip_int,
         NET.IPV4_TO_INT64(NET.IP_FROM_STRING(ip_end)) AS end_ip
         from `k8s-infra-ii-sandbox.k8s_artifacts_dataset_bb_test.shadow_ip_asn_extended`
         WHERE regexp_contains(ip_start, r"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}");
       '

    ## This should be the end of pyasn section, we have results table that covers start_ip/end_ip from fs our requirements
    ## GET k8s asn yaml using:
    ## https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn_k8s_yaml.org
    ## Lets create csv's to import
    ## TODO: refactor this to loop that can generate these in a couple of passes
    curl -s https://raw.githubusercontent.com/kubernetes/k8s.io/main/registry.k8s.io/infra/meta/asns/microsoft.yaml | yq e . -j - \
    | jq -r '.name as $name | .redirectsTo.registry as $redirectsToRegistry | .redirectsTo.artifacts as $redirectsToArtifacts | .asns[] | [.,$name, $redirectsToRegistry, $redirectsToArtifacts] | @csv' > /tmp/microsoft_yaml.csv
    curl -s https://raw.githubusercontent.com/kubernetes/k8s.io/main/registry.k8s.io/infra/meta/asns/google.yaml | yq e . -j - \
    | jq -r '.name as $name | .redirectsTo.registry as $redirectsToRegistry | .redirectsTo.artifacts as $redirectsToArtifacts | .asns[] | [.,$name, $redirectsToRegistry, $redirectsToArtifacts] | @csv' > /tmp/google_yaml.csv
    curl -s https://raw.githubusercontent.com/kubernetes/k8s.io/main/registry.k8s.io/infra/meta/asns/amazon.yaml | yq e . -j - \
    | jq -r '.name as $name | .redirectsTo.registry as $redirectsToRegistry | .redirectsTo.artifacts as $redirectsToArtifacts | .asns[] | [.,$name, $redirectsToRegistry, $redirectsToArtifacts] | @csv' > /tmp/amazon_yaml.csv
    curl -s https://raw.githubusercontent.com/kubernetes/k8s.io/main/registry.k8s.io/infra/meta/asns/alibabagroup.yaml | yq e . -j - \
    | jq -r '.name as $name | .redirectsTo.registry as $redirectsToRegistry | .redirectsTo.artifacts as $redirectsToArtifacts | .asns[] | [.,$name, $redirectsToRegistry, $redirectsToArtifacts] | @csv' > /tmp/alibabagroup_yaml.csv
    curl -s https://raw.githubusercontent.com/kubernetes/k8s.io/main/registry.k8s.io/infra/meta/asns/baidu.yaml | yq e . -j - \
    | jq -r '.name as $name | .redirectsTo.registry as $redirectsToRegistry | .redirectsTo.artifacts as $redirectsToArtifacts | .asns[] | [.,$name, $redirectsToRegistry, $redirectsToArtifacts] | @csv' > /tmp/baidu_yaml.csv
    curl -s https://raw.githubusercontent.com/kubernetes/k8s.io/main/registry.k8s.io/infra/meta/asns/digitalocean.yaml | yq e . -j - \
    | jq -r '.name as $name | .redirectsTo.registry as $redirectsToRegistry | .redirectsTo.artifacts as $redirectsToArtifacts | .asns[] | [.,$name, $redirectsToRegistry, $redirectsToArtifacts] | @csv' > /tmp/digitalocean_yaml.csv
    curl -s https://raw.githubusercontent.com/kubernetes/k8s.io/main/registry.k8s.io/infra/meta/asns/equinixmetal.yaml | yq e . -j - \
    | jq -r '.name as $name | .redirectsTo.registry as $redirectsToRegistry | .redirectsTo.artifacts as $redirectsToArtifacts | .asns[] | [.,$name, $redirectsToRegistry, $redirectsToArtifacts] | @csv' > /tmp/equinixmetal_yaml.csv
    curl -s https://raw.githubusercontent.com/kubernetes/k8s.io/main/registry.k8s.io/infra/meta/asns/huawei.yaml | yq e . -j - \
    | jq -r '.name as $name | .redirectsTo.registry as $redirectsToRegistry | .redirectsTo.artifacts as $redirectsToArtifacts | .asns[] | [.,$name, $redirectsToRegistry, $redirectsToArtifacts] | @csv' > /tmp/huawei_yaml.csv
    curl -s https://raw.githubusercontent.com/kubernetes/k8s.io/main/registry.k8s.io/infra/meta/asns/tencentcloud.yaml | yq e . -j - \
    | jq -r '.name as $name | .redirectsTo.registry as $redirectsToRegistry | .redirectsTo.artifacts as $redirectsToArtifacts | .asns[] | [.,$name, $redirectsToRegistry, $redirectsToArtifacts] | @csv' > /tmp/tencentcloud_yaml.csv

    ## Load all the csv
    ## TODO: Make this into a loop.
    ## TODO: Set a final destination table
    bq load --autodetect k8s_artifacts_dataset_bb_test.k8s_repo_json /tmp/microsoft_yaml.csv
    bq load --autodetect k8s_artifacts_dataset_bb_test.k8s_repo_json /tmp/google_yaml.csv
    bq load --autodetect k8s_artifacts_dataset_bb_test.k8s_repo_json /tmp/amazon_yaml.csv
    bq load --autodetect k8s_artifacts_dataset_bb_test.k8s_repo_json /tmp/alibabagroup_yaml.csv
    bq load --autodetect k8s_artifacts_dataset_bb_test.k8s_repo_json /tmp/baidu_yaml.csv
    bq load --autodetect k8s_artifacts_dataset_bb_test.k8s_repo_json /tmp/digitalocean_yaml.csv
    bq load --autodetect k8s_artifacts_dataset_bb_test.k8s_repo_json /tmp/equinixmetal_yaml.csv
    bq load --autodetect k8s_artifacts_dataset_bb_test.k8s_repo_json /tmp/huawei_yaml.csv
    bq load --autodetect k8s_artifacts_dataset_bb_test.k8s_repo_json /tmp/tencentcloud_yaml.csv

    ## GET Vendor YAML
    ## https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn_k8s_yaml.org
    curl 'https://download.microsoft.com/download/7/1/D/71D86715-5596-4529-9B13-DA13A5DE5B63/ServiceTags_Public_20210607.json' | jq -r \
    '.values[] | .properties.platform as $service | .properties.region as $region | .properties.addressPrefixes[] | [., $service, $region] | @csv' > /tmp/microsoft_subnet_region.csv
    curl 'https://www.gstatic.com/ipranges/cloud.json' | jq -r '.prefixes[] | [.ipv4Prefix, .service, .scope] | @csv' > /tmp/google_raw_subnet_region.csv
    curl 'https://ip-ranges.amazonaws.com/ip-ranges.json' | jq -r '.prefixes[] | [.ip_prefix, .service, .region] | @csv' > /tmp/amazon_raw_subnet_region.csv

    ## Load all the csv
    ## TODO: Make this into a loop.
    ## TODO: Set a final destination table
    bq load --autodetect k8s_artifacts_dataset_bb_test.amazon_raw_subnet_region /tmp/amazon_raw_subnet_region.csv
    bq load --autodetect k8s_artifacts_dataset_bb_test.google_raw_subnet_region /tmp/google_raw_subnet_region.csv
    bq load --autodetect k8s_artifacts_dataset_bb_test.google_raw_subnet_region /tmp/microsoft_subnet_region.csv

    ## GET Metadata from peeringdb
    ## https://github.com/ii/org/blob/main/research/asn-data-pipeline/etl_asn_metadata_table.org
    ##


















#+end_src



#+begin_src shell
chmod +x /tmp/main_etl_processor.sh
. /tmp/main_etl_processor.sh
#+end_src

#+RESULTS:
#+begin_example
hi me
#+end_example
