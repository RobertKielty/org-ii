#+TITLE: K8s Reg Asn Magic
#+PROPERTY: header-args:sql-mode+ :eval never-export :exports both :session none

Login to gcloud
#+BEGIN_SRC tmate :window prepare
gcloud auth login
#+END_SRC

Set the project
#+BEGIN_SRC tmate :window prepare
gcloud config set project k8s-infra-ii-sandbox
#+END_SRC

* Parse from API
Save the data to a bucket
#+BEGIN_SRC tmate :window prepare
bq extract --destination_format NEWLINE_DELIMITED_JSON k8s_artifacts_gcslogs_appspot.riaan_ipv4_asn_ip_name_over_2500 gs://ii_bq_scratch_dump/ip-and-asn.json
#+END_SRC

Download the data from the bucket
#+BEGIN_SRC tmate :window prepare
gsutil cp gs://ii_bq_scratch_dump/ip-and-asn.json ip-and-asn.json
#+END_SRC

Store only the ASN
#+BEGIN_SRC tmate :window prepare
cat ip-and-asn.json | jq -r '.asn' | sort | uniq > asns.txt
#+END_SRC

Formatting the data
#+BEGIN_SRC shell :tangle ./asn-data-processor.sh :results silent
#!/bin/bash

SKIP_TO=$1
READ_FROM=asns.txt
WRITE_TO=asn-data.csv

TMPDIR=$(mktemp -d)
echo "Temp folder: $TMPDIR"

ALLOWED_RETRIES=5

count=0
while IFS= read -r asn; do
    count=$((count+=1))
    retries=0
    echo "ASN[$count]: $asn"
    if [ $asn -eq 0 ] || ( [ ! -z $SKIP_TO ] && [ $count -lt $SKIP_TO ] ); then
        echo "Skipping [$count] $asn"
        continue
    fi
    until curl "https://api.bgpview.io/asn/$asn" 2> /dev/null > $TMPDIR/$asn.json && cat $TMPDIR/$asn.json | jq .data.name 2>&1 > /dev/null; do
        retries=$((retries+=1))
        if [ $retries -eq $ALLOWED_RETRIES ]; then
            echo "Skipping [$count] $asn"
            retries=0
            continue 2
        fi
        echo "Failed [$retries/$ALLOWED_RETRIES]. Retrying '$asn' in 3 seconds"
        sleep 3s
    done
    cat $TMPDIR/$asn.json | jq -r '.data | (.email_contacts | join(";")) as $contacts | .description_short as $name | [.asn, $name, $contacts] | @csv' 2> /dev/null \
        | tee -a $WRITE_TO 2>&1 > /dev/null
    sleep 1s
done < $READ_FROM
#+END_SRC

Run the script
#+BEGIN_SRC tmate :window prepare
chmod +x ./asn-data-processor.sh
time ./asn-data-processor.sh
#+END_SRC

Upload to the bucket
#+BEGIN_SRC shell :results silent
gsutil cp ./asn-data.csv gs://ii_bq_scratch_dump/asn-data.csv
gsutil ls $_
#+END_SRC

Load into big query
#+BEGIN_SRC shell :results silent
bq load --autodetect --source_format=CSV k8s_artifacts_gcslogs_appspot.asn_company_lookup gs://ii_bq_scratch_dump/asn-data.csv
#+END_SRC

* Parse from Postgres

Bring up Postgres
#+BEGIN_SRC tmate :window postgres
docker run -it --rm -p 5432:5432 -e POSTGRES_PASSWORD=password -e POSTGRES_DB=peeringdb postgres:12.2-alpine
#+END_SRC

Clone https://git.2e8.dk/peeringdb-simplesync
#+BEGIN_SRC tmate :window prepare :dir (getenv "HOME")
git clone https://git.2e8.dk/peeringdb-simplesync
cd peeringdb-simplesync
#+END_SRC

Enter PeeringDB creds
#+BEGIN_SRC tmate :window prepare :dir (concat (getenv "HOME") "/peeringdb-simplesync")
read -p 'PEERINGDB_USER    : ' PEERINGDB_USER
read -p 'PEERINGDB_PASSWORD: ' PEERINGDB_PASSWORD
#+END_SRC

Write the config for sync.py
#+BEGIN_SRC python :tangle (concat (getenv "HOME") "/peeringdb-simplesync/config.py")
from requests.auth import HTTPBasicAuth
import os

host=os.environ['SHARINGIO_PAIR_LOAD_BALANCER_IP']
user=os.environ['PEERINGDB_USER']
password=os.environ['PEERINGDB_PASSWORD']

def get_config():
    return {
        'db_conn_str': 'dbname=peeringdb host=%s user=postgres password=password' % host,
        'db_schema': 'peeringdb',
        'auth': HTTPBasicAuth('%s' % user, '%s' % password),
    }
#+END_SRC

Dump all of the data
#+BEGIN_SRC tmate :window peeringdb-sync :dir (concat (getenv "HOME") "/peeringdb-simplesync")
./sync
#+END_SRC

Set env vars to not prompt for Postgres username and password
#+BEGIN_SRC tmate :window peeringdb-sync :dir (concat (getenv "HOME") "/peeringdb-simplesync")
export \
    PGUSER=postgres \
    PGPASSWORD=password
#+END_SRC

** Create a new dump
Dump the database
#+BEGIN_SRC tmate :window peeringdb-sync :dir (concat (getenv "HOME") "/peeringdb-simplesync")
pg_dump -U postgres -d peeringdb -h $SHARINGIO_PAIR_LOAD_BALANCER_IP > peeringdb-dump-$(date +%Y%m%d).sql
#+END_SRC

Upload the dump
#+BEGIN_SRC tmate :window peeringdb-sync
gsutil cp peeringdb-dump-$(date +%Y%m%d).sql gs://ii_bq_scratch_dump/peeringdb-dump-$(date +%Y%m%d).sql
#+END_SRC

** With pre-prepared dump

Download from the bucket
#+BEGIN_SRC tmate :window peeringdb-sync
gsutil cp gs://ii_bq_scratch_dump/peeringdb-dump-20210512.sql ./peeringdb-dump-20210512.sql
#+END_SRC

Load the data from the dump into a new/separate Postgres instance
#+BEGIN_SRC tmate :window peeringdb-sync
psql -U postgres -d peeringdb -h $SHARINGIO_PAIR_LOAD_BALANCER_IP < ./peeringdb-dump-20210512.sql
#+END_SRC

** Explore

Connect with psql
#+BEGIN_SRC tmate :window peeringdb-sync
psql -U postgres -d peeringdb -h $SHARINGIO_PAIR_LOAD_BALANCER_IP
#+END_SRC

See the tables
#+BEGIN_SRC sql-mode :eval never-export :exports both :session none :sql-user postgres :sql-database peeringdb :sql-server (getenv "SHARINGIO_PAIR_LOAD_BALANCER_IP") :sql-password password
SELECT schemaname, tablename FROM pg_catalog.pg_tables WHERE schemaname != 'pg_catalog' AND schemaname != 'information_schema';
#+END_SRC

#+RESULTS:
#+begin_SRC example
 schemaname | tablename
------------+-----------
 peeringdb  | fac
 peeringdb  | ix
 peeringdb  | ixfac
 peeringdb  | ixlan
 peeringdb  | ixpfx
 peeringdb  | net
 peeringdb  | netfac
 peeringdb  | netixlan
 peeringdb  | org
 peeringdb  | poc
(10 rows)

#+end_SRC

Find data from peeringdb.org table
#+BEGIN_SRC sql-mode
select id, data::jsonb ->> 'name' as name, data::jsonb ->> 'asn' as asn, data::jsonb ->> 'website' as "website" from peeringdb.org where 'website' is not null limit 5;
#+END_SRC

Find data from peeringdb.net table
#+BEGIN_SRC sql-mode
select id, data::jsonb ->> 'name' as name, data::jsonb ->> 'asn' as asn, data::jsonb ->> 'website' as "website" from peeringdb.net limit 5;
#+END_SRC

Getting fields with emails
#+BEGIN_SRC sql-mode
select id, data::jsonb ->> 'name' as name, data::jsonb ->> 'email' as email, net_id from peeringdb.poc where status = 'ok' limit 5;
#+END_SRC

Connect ASNs with emails by joining names between tables
#+BEGIN_SRC sql-mode
select net.id,
       (net.data ->> 'name') as "name",
       (net.data ->> 'asn') as "asn",
       (net.data ->> 'website') as website,
       (poc.data ->> 'email') as email
       from peeringdb.net net
       left join peeringdb.poc on ((peeringdb.poc.data ->> 'name') = net.data ->> 'name')
       where (net.data ->>'website') is not null
       order by email asc
       limit 5;
#+END_SRC

#+BEGIN_SRC sql-mode
\d peeringdb.net
#+END_SRC

** Building with Postgres
#+BEGIN_SRC sql-mode
create schema asntocompany;
#+END_SRC

#+RESULTS:
#+begin_SRC example
ERROR:  schema "asntocompany" already exists
#+end_SRC
#+BEGIN_SRC sql-mode
create table asnproc (
       asn bigint not null primary key
);
\copy asnproc from '/home/ii/peeringdb-simplesync/asns.txt';
#+END_SRC

#+RESULTS:
#+begin_SRC example
CREATE TABLE
COPY 415
#+end_SRC

#+BEGIN_SRC sql-mode
select (net.data ->> 'name') as "name",
       asn
    from peeringdb.net
    where (net.data ->> 'name') ilike '%google%'
    limit 5;
#+END_SRC

#+BEGIN_SRC sql-mode
select count(*)
from peeringdb.poc p
where (p.data ->> 'email') is not null;
#+END_SRC

#+RESULTS:
#+begin_SRC example
 count
-------
 10756
(1 row)

#+end_SRC

#+BEGIN_SRC sql-mode
select asn.asn,
       (net.data ->> 'name') as "name",
       (net.data ->> 'website') as "website",
       (poc.data ->> 'email') as email
       from asnproc asn
       left join peeringdb.net net on (net.asn = asn.asn)
       left join peeringdb.poc poc on ((poc.data ->> 'name') = (net.data ->> 'name'))
       -- where (net.data ->>'website') is not null
       -- where (poc.data ->> 'email') is not null
       order by email asc;
#+END_SRC

#+BEGIN_SRC sql-mode
select
       (poc.data ->> 'name') as poc_name
from peeringdb.poc poc
-- left join peeringdb.poc poc on ((net.data ->>'name') = (poc.data ->>'name'))
where (poc.data ->> 'name') ilike '%google%'
or (poc.data ->> 'name') ilike '%amazon%'
or (poc.data ->> 'name') ilike '%microsoft%';
-- where (net.data ->>'name') ilike '%google%';
-- select data from peeringdb.net where (data ->> 'asn')::bigint = 21789 limit 1;
#+END_SRC

#+BEGIN_SRC sql-mode
begin;
-- create table asnproc (
--        asn bigint not null primary key
-- );
-- \copy asnproc from '/home/ii/peeringdb-simplesync/asns.txt';
select count(*) from peeringdb.poc;
select net.id,
       asnproc.asn,
       (net.data ->> 'name') as "name",
       (net.data ->> 'website') as "website"
       -- (poc.data ->> 'email') as email
       from asnproc
       join peeringdb.net net on ((net.data ->> 'asn')::bigint = asnproc.asn)
       -- left join peeringdb.poc poc on ((poc.data ->> 'name') = 'chonkers')
       -- left join peeringdb.poc poc on ((poc.data ->> 'name') = (net.data ->> 'name'))
       -- where (net.data ->>'website') is not null
       -- order by email asc
       limit 5;
rollback;
#+END_SRC

** Building with Go

Scripting the data fetching in Go
#+BEGIN_SRC go :tangle ./asn-db-data-processor.go
package main

import (
	"fmt"
	"log"
	"os"
	"database/sql"
	_ "github.com/lib/pq"
)

type asnToCompany struct {
	ID string
	Name string
	ASN string
	Email string
}

type asnToCompanySet []asnToCompany

func GetDBConnection() (*sql.DB, error) {
	db, err := sql.Open("postgres", fmt.Sprintf("postgres://postgres:password@%v/peeringdb", os.Getenv("SHARINGIO_PAIR_LOAD_BALANCER_IP")))
	db.Ping()
	return db, err
}

func main() {
	db, err := GetDBConnection()
	if err != nil {
		log.Fatalln(err)
	}
	db.Ping()
}
#+END_SRC

* Clean up
Remove the table
#+BEGIN_SRC shell
bq rm k8s_artifacts_gcslogs_appspot.asn_company_lookup
#+END_SRC

Clean up
#+BEGIN_SRC shell :results silent
rm -f asn-data.csv
#+END_SRC
