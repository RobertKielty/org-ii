#+TITLE: Local cluster
#+PROPERTY: header-args:shell+ :prologue "( " :epilogue " ) 2>&1 ; :"
#+PROPERTY: header-args:yaml+ :comments none
#+PROPERTY: header-args:patch+ :comments none

Setting up a local cluster.

* Prologue


* Prepare
** Save repo location
#+begin_src tmate :window prepare
export REPO_ROOT="${PWD}"
#+end_src

** Downloading a Talos RPi image
Download the Talos image to flash to a MicroSD card from GitHub
#+begin_src tmate :window prepare
cd $(mktemp -d)
curl -O -L \
  https://github.com/talos-systems/talos/releases/download/v0.10.3/metal-rpi_4-arm64.img.xz
export TALOS_METAL_RPI_IMG=${PWD}/*
#+end_src

Some Pis may require having the EEPROM updated, check [[https://www.talos.dev/docs/v0.10/single-board-computers/rpi_4/#updating-the-eeprom][the Talos docs]].

** Prepare MicroSD cards
Write the image to a MicroSD card
#+begin_src tmate :window prepare
export DISK_TO_USE_DEFAULT=/dev/sdb && \
  read -p "Enter the disk to use (default: '${DISK_TO_USE}'): " DISK_TO_USE && \
  sudo dd \
    if=${TALOS_METAL_RPI_IMG} \
    of="${DISK_TO_USE-$DISK_TO_USE_DEFAULT}" \
    status=progress \
    conv=fsync \
    bs=4M
#+end_src

** Install =talosctl=
To manage Talos on each node, =talosctl= is used to provision and manage
#+begin_src tmate :window prepare
curl -o ~/bin/talosctl -L \
  https://github.com/talos-systems/talos/releases/download/v0.10.3/talosctl-$(uname | tr '[:upper:]' '[:lower:]')-amd64
#+end_src

* Set up
** Discover node IPs
Each node that comes up will, of course, have an IP address.
I'm checking what the router says.

** Determine that nodes are live
#+begin_src tmate :window prepare
export NODE_ADDRS=(192.168.1.111 192.168.1.127 192.168.1.234)
#+end_src

** Ensure nodes are live
#+begin_src tmate :window prepare
for IP in ${NODE_ADDRS[*]}; do
    echo "Checking ${IP}:50000"
    nc -zv "${IP}" "50000"
done
#+end_src

** Generating the configuration
#+begin_src tmate :window prepare
talosctl gen config \
    ii-nz \
    https://192.168.1.100:6443 \
    --output-dir talos/ \
    --additional-sans k8s.ii.nz \
    --install-disk /dev/mmcblk0 \
    --install-image ghcr.io/talos-systems/installer:v0.10.3
#+end_src

** Modify the configuration
#+begin_src diff :tangle talos-config-patches.patch :comment none
diff --git a/talos/controlplane.yaml b/talos/controlplane.yaml
index bc87738..cf17a8a 100644
--- a/talos/controlplane.yaml
+++ b/talos/controlplane.yaml
@@ -35,7 +35,12 @@ machine:
     #         - rw

     # Provides machine specific network configuration options.
-    network: {}
+    network:
+      interfaces:
+        - interface: eth0
+          dhcp: true
+          vip:
+            ip: 192.168.1.100
     # # `interfaces` is used to define the network interface configuration.
     # interfaces:
     #     - interface: eth0 # The interface name.
@@ -214,6 +219,7 @@ machine:
     #               slot: 0 # Key slot number for luks2 encryption.
 # Provides cluster specific configuration options.
 cluster:
+    allowSchedulingOnMasters: true
     # Provides control plane specific configuration options.
     controlPlane:
         endpoint: https://192.168.1.100:6443 # Endpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.
diff --git a/talos/init.yaml b/talos/init.yaml
index ad6d34e..46bdafd 100644
--- a/talos/init.yaml
+++ b/talos/init.yaml
@@ -35,7 +35,12 @@ machine:
     #         - rw

     # Provides machine specific network configuration options.
-    network: {}
+    network:
+      interfaces:
+        - interface: eth0
+          dhcp: true
+          vip:
+            ip: 192.168.1.100
     # # `interfaces` is used to define the network interface configuration.
     # interfaces:
     #     - interface: eth0 # The interface name.
@@ -214,6 +219,7 @@ machine:
     #               slot: 0 # Key slot number for luks2 encryption.
 # Provides cluster specific configuration options.
 cluster:
+    allowSchedulingOnMasters: true
     # Provides control plane specific configuration options.
     controlPlane:
         endpoint: https://192.168.1.100:6443 # Endpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.

#+end_src

Apply patches
#+begin_src tmate :window prepare
patch -ruN -d talos/ < "${REPO_ROOT}/talos-config-patches.patch"
#+end_src

** Use talosconfig
#+begin_src tmate :window prepare
export TALOSCONFIG=$PWD/talos/talosconfig
#+end_src

Write the endpoint
#+begin_src shell :results silent
talosctl config endpoint 192.168.1.100
#+end_src

** Provisioning the first node
#+begin_src tmate :window prepare
talosctl apply-config --insecure --nodes "${NODE_ADDRS[0]}" --file talos/init.yaml
#+end_src

*** Ensure that the node is active
#+begin_src tmate :window prepare
talosctl health -e "${NODE_ADDRS[0]}" -n "${NODE_ADDRS[0]}"
#+end_src

** Provision all the nodes
#+begin_src tmate :window prepare
for IP in ${NODE_ADDRS[*]}; do
    talosctl apply-config --insecure --nodes "${IP}" --file talos/controlplane.yaml
done
#+end_src

*** Watch the health of all nodes, as they become active
#+begin_src tmate :window prepare
talosctl health -e "${NODE_ADDRS[0]}" -n "${NODE_ADDRS[0]}"
#+end_src

** Get kubeconfig
#+begin_src tmate :window prepare
talosctl kubeconfig -e 192.168.1.100 -n 192.168.1.100
#+end_src

** Get nodes
#+begin_src shell
kubectl get nodes
#+end_src

#+RESULTS:
#+begin_example
NAME                  STATUS   ROLES                  AGE     VERSION
talos-192-168-1-111   Ready    control-plane,master   16m     v1.21.1
talos-192-168-1-127   Ready    control-plane,master   8m2s    v1.21.1
talos-192-168-1-234   Ready    control-plane,master   7m43s   v1.21.1
#+end_example

* Validate
** Get pods
#+begin_src shell
kubectl get pods -A
#+end_src

#+RESULTS:
#+begin_example
NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE
kube-system   coredns-fcc4c97fb-br6rd                       1/1     Running   0          17m
kube-system   coredns-fcc4c97fb-cfstz                       1/1     Running   0          17m
kube-system   kube-apiserver-talos-192-168-1-111            1/1     Running   0          14m
kube-system   kube-apiserver-talos-192-168-1-127            1/1     Running   0          7m23s
kube-system   kube-apiserver-talos-192-168-1-234            1/1     Running   0          7m55s
kube-system   kube-controller-manager-talos-192-168-1-111   1/1     Running   3          15m
kube-system   kube-controller-manager-talos-192-168-1-127   1/1     Running   0          7m23s
kube-system   kube-controller-manager-talos-192-168-1-234   1/1     Running   0          7m55s
kube-system   kube-flannel-5stx9                            1/1     Running   0          8m16s
kube-system   kube-flannel-9kcx2                            1/1     Running   0          7m56s
kube-system   kube-flannel-wxn5m                            1/1     Running   0          16m
kube-system   kube-proxy-6dzrl                              1/1     Running   0          7m56s
kube-system   kube-proxy-pb42s                              1/1     Running   0          8m16s
kube-system   kube-proxy-w5q56                              1/1     Running   0          16m
kube-system   kube-scheduler-talos-192-168-1-111            1/1     Running   3          15m
kube-system   kube-scheduler-talos-192-168-1-127            1/1     Running   0          7m23s
kube-system   kube-scheduler-talos-192-168-1-234            1/1     Running   0          7m55s
#+end_example

* Ensure set up
** Upload talos folder into Kubernetes secret
#+begin_src tmate :window prepare
kubectl -n kube-system create secret generic "talos-config" --from-file=talos/
#+end_src

Ensure that the files exist in the secret
#+begin_src shell
kubectl -n kube-system get secret talos-config -o yaml | yq e '.data | keys | .[]' -P -
#+end_src

#+RESULTS:
#+begin_example
controlplane.yaml
init.yaml
join.yaml
talosconfig
#+end_example

** Fetch Talos configs
Create a new temp directory
#+begin_src tmate :window prepare
cd $(mktemp -d)
#+end_src

Extract talos-config into directory
#+begin_src tmate :window prepare
TALOS_CONFIGS="$(mktemp -t talos-config-XXXXX)"
kubectl -n kube-system get secret talos-config -o yaml > "${TALOS_CONFIGS}"

mkdir -p talos/
for FILE in $(cat "${TALOS_CONFIGS}" | yq e '.data | keys | .[]' -P -); do
  echo $FILE
  cat "${TALOS_CONFIGS}" | yq e ".data.\"${FILE}\"" -P - | base64 --decode > "talos/${FILE}"
done
#+end_src

** Get node IPs from the cluster
#+begin_src tmate :window prepare
export NODE_ADDRS=$(kubectl get nodes -o yaml | yq e '.items[].status.addresses[] | select(.type=="InternalIP") | .address' -P -)
#+end_src

** Get machinetype
#+begin_src tmate :window prepare
talosctl -e 192.168.1.100 -n "$(echo ${NODE_ADDRS} | tr ' ' ',')" get machinetype
#+end_src

** Shutdown RPis
#+begin_src tmate :window prepare
for IP in ${NODE_ADDRS[*]}; do
    talosctl shutdown -e 192.168.1.100 -n "${IP}"
done
#+end_src

** Reset all nodes to uninitialised Talos
#+begin_src tmate :window prepare
read -p "Are you sure you want to reset all nodes, effectively destroying the cluster? [Enter|C-c] " && \
(
  for IP in ${NODE_ADDRS[*]}; do
      talosctl -e "${IP}" -n "${IP}" reset --graceful=false --reboot --system-labels-to-wipe=EPHEMERAL
  done
)
#+end_src

* Workloads
** metallb
*** Prepare
Create a directory for the manifests and a namespace for the resources
#+begin_src shell :results silent
mkdir -p metallb
curl -o metallb/namespace.yaml -L https://raw.githubusercontent.com/metallb/metallb/v0.9.6/manifests/namespace.yaml
curl -o metallb/metallb.yaml -L https://raw.githubusercontent.com/metallb/metallb/v0.9.6/manifests/metallb.yaml
#+end_src

*** Configure
Using layer2 for ARP capabilities and provide a very sufficient 10 IP address range in a part of the network that is configure to not be used by DHCP.
#+begin_src yaml :tangle ./metallb/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 192.168.1.20-192.168.1.30
#+end_src

*** Install
#+begin_src shell
kubectl apply -f metallb/namespace.yaml
kubectl -n metallb-system get secret memberlist 2> /dev/null \
    || kubectl -n metallb-system create secret generic memberlist --from-literal=secretkey="$(openssl rand -base64 128)"
kubectl -n metallb-system apply -f ./metallb/config.yaml
kubectl -n metallb-system apply -f ./metallb/metallb.yaml
#+end_src

#+RESULTS:
#+begin_example
namespace/metallb-system created
secret/memberlist created
configmap/config created
Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
podsecuritypolicy.policy/controller created
podsecuritypolicy.policy/speaker created
serviceaccount/controller created
serviceaccount/speaker created
clusterrole.rbac.authorization.k8s.io/metallb-system:controller created
clusterrole.rbac.authorization.k8s.io/metallb-system:speaker created
role.rbac.authorization.k8s.io/config-watcher created
role.rbac.authorization.k8s.io/pod-lister created
clusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created
clusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created
rolebinding.rbac.authorization.k8s.io/config-watcher created
rolebinding.rbac.authorization.k8s.io/pod-lister created
daemonset.apps/speaker created
deployment.apps/controller created
#+end_example

** Helm-Operator
Unfortunately the Helm-Operator project by FluxCD is both in maintenance mode and unsupported on arm64. Here in the prepare stage, I'm patching the current state of how things are to build an arm64 image. Ideally, this is all in a single Dockerfile and does not use Make scripts. I'm unsure what the future of Helm-Operator is, but I'd like to see and help support for architectures outta-the-box.

*** Prepare
Create a directory for the manifests and a namespace for the resources
#+begin_src shell :results silent
mkdir -p helm-operator
kubectl create namespace helm-operator --dry-run=client -o yaml \
  | kubectl apply -f -
#+end_src

*** Configure
Create local manifests to apply in the cluster
#+begin_src shell :results silent
curl -o ./helm-operator/helm-operator-crds.yaml -L https://raw.githubusercontent.com/fluxcd/helm-operator/1.2.0/deploy/crds.yaml

helm repo add fluxcd https://charts.fluxcd.io
helm template helm-operator --create-namespace fluxcd/helm-operator \
    --namespace helm-operator \
    --set helm.versions=v3 \
    --set image.repository=registry.gitlab.com/bobymcbobs/container-images/helm-operator \
    --set image.tag=1.2.0 \
      > ./helm-operator/helm-operator.yaml
#+end_src

*** Install
#+begin_src shell
kubectl apply -f ./helm-operator/helm-operator-crds.yaml
kubectl -n helm-operator apply -f ./helm-operator/helm-operator.yaml
#+end_src

#+RESULTS:
#+begin_example
Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition
customresourcedefinition.apiextensions.k8s.io/helmreleases.helm.fluxcd.io created
serviceaccount/helm-operator created
secret/helm-operator-git-deploy created
configmap/helm-operator-kube-config created
Warning: rbac.authorization.k8s.io/v1beta1 ClusterRole is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRole
clusterrole.rbac.authorization.k8s.io/helm-operator created
Warning: rbac.authorization.k8s.io/v1beta1 ClusterRoleBinding is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRoleBinding
clusterrolebinding.rbac.authorization.k8s.io/helm-operator created
service/helm-operator created
deployment.apps/helm-operator created
#+end_example

** nginx-ingress controller
*** Prepare

Create a directory for the manifests and a namespace for the resources
#+begin_src shell :results silent
mkdir -p nginx-ingress
kubectl create namespace nginx-ingress --dry-run=client -o yaml \
  | kubectl apply -f -
#+end_src

*** Configure
Ensuring that remote IP addresses will be forwarded as headers in the requests, using the fields in the /.spec.values.controller.service/ field.
Preferring that each nginx-ingress pod runs on a different node.
#+begin_src yaml :tangle ./nginx-ingress/nginx-ingress.yaml
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: nginx-ingress
  namespace: nginx-ingress
spec:
  releaseName: nginx-ingress
  chart:
    repository: https://kubernetes.github.io/ingress-nginx
    name: ingress-nginx
    version: 3.30.0
  values:
    controller:
      autoscaling:
        enabled: true
        minReplicas: 3
        maxReplicas: 10
        targetCPUUtilizationPercentage: 80
      service:
        type: LoadBalancer
        externalTrafficPolicy: Local
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                      - ingress-nginx
              topologyKey: "kubernetes.io/hostname"
    defaultBackend:
      enabled: false
#+end_src

*** Install
#+begin_src shell
kubectl -n nginx-ingress apply -f nginx-ingress/nginx-ingress.yaml
#+end_src

#+RESULTS:
#+begin_example
helmrelease.helm.fluxcd.io/nginx-ingress configured
#+end_example

** Cert-Manager
*** Prepare
#+begin_src shell :results silent
mkdir -p ./cert-manager
curl -o ./cert-manager/cert-manager.yaml -L https://github.com/jetstack/cert-manager/releases/download/v1.4.0/cert-manager.yaml
#+end_src

*** Install
#+begin_src shell :results silent
kubectl apply -f ./cert-manager/cert-manager.yaml
#+end_src

** CAPI + Sidero
Links:
- https://www.sidero.dev/docs/v0.3/getting-started/install-clusterapi/
- https://www.sidero.dev/docs/v0.3/guides/rpi4-as-servers/#rpi4-boot-process

*** Install
#+begin_src tmate :window prepare
export SIDERO_CONTROLLER_MANAGER_HOST_NETWORK=true
export SIDERO_CONTROLLER_MANAGER_API_ENDPOINT=192.168.1.21

clusterctl init -b talos -c talos -i sidero
#+end_src

** PXE boot server (dnsmasq)
*** Prepare
#+begin_src shell :results silent
mkdir -p dnsmasq
kubectl create namespace dnsmasq --dry-run=client -o yaml | \
    kubectl apply -f -
#+end_src

*** Configure

Configure dnsmasq
#+begin_src text :tangle ./dnsmasq/dnsmasq.conf :comments none
#dnsmasq config, for a complete example, see:
#  http://oss.segetech.com/intra/srv/dnsmasq.conf

port=0
dhcp-range=192.168.1.0,proxy

pxe-service=0,"Raspberry Pi Boot"
#+end_src

Configure the container
#+begin_src dockerfile :tangle ./dnsmasq/Dockerfile :comments none
FROM alpine:edge
# fetch dnsmasq and webproc binary
RUN apk update \
 && apk --no-cache add dnsmasq tcpdump git \
 && apk add --no-cache --virtual curl \
#configure dnsmasq
RUN mkdir -p /etc/default/
RUN echo -e "ENABLED=1\nIGNORE_RESOLVCONF=yes" > /etc/default/dnsmasq
#run!
ENTRYPOINT ["dnsmasq","--no-daemon"]
#+end_src

Configure the deployment
#+begin_src yaml :tangle ./dnsmasq/dnsmasq.yaml :comments none
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dnsmasq
  namespace: dnsmasq
  labels:
    nz.ii: dnsmasq
    app: dnsmasq
spec:
  strategy:
    type: Recreate
  replicas: 3
  selector:
    matchLabels:
      nz.ii: dnsmasq
  template:
    metadata:
      annotations:
        nz.ii/dnsmasq.conf-sha256sum: "${DNSMASQ_CONF_HASH}"
        nz.ii/dockerfile-sha256sum: "${DOCKERFILE_HASH}"
      labels:
        nz.ii: dnsmasq
        app: dnsmasq
    spec:
      hostNetwork: true
      containers:
      - name: dnsmasq
        image: registry.gitlab.com/ii/nz/dnsmasq:latest
        imagePullPolicy: Always
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
              - NET_ADMIN
              - NET_RAW
              - SYS_ADMIN
          privileged: true
        ports:
        - containerPort: 67
          hostPort: 67
          protocol: UDP
#+end_src

*** Build
#+begin_src tmate :window dnsmasq
kubectl build \
    --destination registry.gitlab.com/ii/nz/dnsmasq:latest \
    --snapshotMode=redo \
    --context=$PWD \
    --dockerfile ./dnsmasq/Dockerfile
#+end_src

*** Install
#+begin_src shell
kubectl -n dnsmasq create configmap dnsmasq-config --from-file=dnsmasq/dnsmasq.conf --dry-run=client -o yaml | \
    kubectl apply -f -
export DNSMASQ_CONF_HASH="$(sha256sum ./dnsmasq/dnsmasq.conf | awk '{print $1}')"
export DOCKERFILE_HASH="$(sha256sum ./dnsmasq/Dockerfile | awk '{print $1}')"
envsubst < ./dnsmasq/dnsmasq.yaml | kubectl apply -f -
#+end_src

#+RESULTS:
#+begin_example
configmap/dnsmasq-config configured
deployment.apps/dnsmasq configured
#+end_example

*** Validate
#+begin_src tmate :window prepare
for IP in ${NODE_ADDRS[*]}; do
    echo "Checking ${IP}:67"
    nc -zvu "${IP}" "67"
done
#+end_src
